{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1215da50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping torchvison as it is not installed.\u001b[0m\n",
      "yes: standard output: Broken pipe\n",
      "yes: write error\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.5.1 requires torch<1.10,>=1.7.0, but you have torch 1.10.1 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!yes | pip uninstall torchvison\n",
    "!pip install -qU torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc6fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10bf914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket : sagemaker-eu-west-3-543553163241/sagemaker/rain-au-pytorch-notebook \n",
      " Role : arn:aws:iam::543553163241:role/service-role/AmazonSageMaker-ExecutionRole-20220116T155750\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/rain-au-pytorch-notebook'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"Bucket : {bucket}/{prefix} \\n Role : {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61253b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"pipelines/rain/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a8678ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_job_prefix=\"RainAu\"\n",
    "\n",
    "processing_instance_count = 1\n",
    "processing_instance_type = \"ml.m5.xlarge\"\n",
    "training_instance_type = \"ml.m5.xlarge\"\n",
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "inference_instance_count = 1\n",
    "model_approval_status = \"PendingManualApproval\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a119f1",
   "metadata": {},
   "source": [
    "# Getting and preprocessing the data\n",
    "Data is stored in an s3 bucket in the account, needs to be processed by an SKlearn processor.\n",
    "\n",
    "First let us update the git repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8807a40c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 9, done.\u001b[K\r\n",
      "remote: Counting objects:  11% (1/9)\u001b[K\r",
      "remote: Counting objects:  22% (2/9)\u001b[K\r",
      "remote: Counting objects:  33% (3/9)\u001b[K\r",
      "remote: Counting objects:  44% (4/9)\u001b[K\r",
      "remote: Counting objects:  55% (5/9)\u001b[K\r",
      "remote: Counting objects:  66% (6/9)\u001b[K\r",
      "remote: Counting objects:  77% (7/9)\u001b[K\r",
      "remote: Counting objects:  88% (8/9)\u001b[K\r",
      "remote: Counting objects: 100% (9/9)\u001b[K\r",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (1/1)\u001b[K\r",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\r\n",
      "remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0\u001b[K\r\n",
      "Unpacking objects:  20% (1/5)\r",
      "Unpacking objects:  40% (2/5)\r",
      "Unpacking objects:  60% (3/5)\r",
      "Unpacking objects:  80% (4/5)\r",
      "Unpacking objects: 100% (5/5)\r",
      "Unpacking objects: 100% (5/5), 462 bytes | 231.00 KiB/s, done.\r\n",
      "From https://github.com/gmguarino/sagemaker-pipeline-rain-australia-build\r\n",
      " * branch            main       -> FETCH_HEAD\r\n",
      "   dd602bc..69d1482  main       -> origin/main\r\n",
      "Updating dd602bc..69d1482\r\n",
      "Fast-forward\r\n",
      " pipelines/rain/evaluate.py | 10 \u001b[32m+++++\u001b[m\u001b[31m-----\u001b[m\r\n",
      " 1 file changed, 5 insertions(+), 5 deletions(-)\r\n"
     ]
    }
   ],
   "source": [
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0369de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_source = \"s3://rain-data-17012022/data/weatherAUS.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c86ced91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f7220",
   "metadata": {},
   "source": [
    "Examine the preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50405f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcompose\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ColumnTransformer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mimpute\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SimpleImputer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpipeline\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Pipeline\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StandardScaler, OneHotEncoder, LabelEncoder, StandardScaler\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\r\n",
      "\r\n",
      "logger = logging.getLogger()\r\n",
      "logger.setLevel(logging.INFO)\r\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmerge_two_dicts\u001b[39;49;00m(x, y):\r\n",
      "    \u001b[33m\"\"\"Merges two dicts, returning a new copy.\"\"\"\u001b[39;49;00m\r\n",
      "    z = x.copy()\r\n",
      "    z.update(y)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m z\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcyclical_encode\u001b[39;49;00m(data, col, max_val):\r\n",
      "    data[col + \u001b[33m'\u001b[39;49;00m\u001b[33m_sin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = np.sin(\u001b[34m2\u001b[39;49;00m * np.pi * data[col]/max_val)\r\n",
      "    data[col + \u001b[33m'\u001b[39;49;00m\u001b[33m_cos\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = np.cos(\u001b[34m2\u001b[39;49;00m * np.pi * data[col]/max_val)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m data\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting preprocessing.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# parser = argparse.ArgumentParser()\u001b[39;49;00m\r\n",
      "    \u001b[37m# parser.add_argument(\"--input-data\", type=str, required=True)\u001b[39;49;00m\r\n",
      "    \u001b[37m# args = parser.parse_args()\u001b[39;49;00m\r\n",
      "\r\n",
      "    base_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[37m# pathlib.Path(f\"{base_dir}/data\").mkdir(parents=True, exist_ok=True)\u001b[39;49;00m\r\n",
      "    \u001b[37m# input_data = args.input_data\u001b[39;49;00m\r\n",
      "    \u001b[37m# bucket = input_data.split(\"/\")[2]\u001b[39;49;00m\r\n",
      "    \u001b[37m# key = \"/\".join(input_data.split(\"/\")[3:])\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[37m# logger.debug(\"Downloading data from bucket: %s, key: %s\", bucket, key)\u001b[39;49;00m\r\n",
      "    \u001b[37m# fn = f\"{base_dir}/data/rain-au-dataset.csv\"\u001b[39;49;00m\r\n",
      "    fn = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/input/weatherAUS.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[37m# s3 = boto3.resource(\"s3\")\u001b[39;49;00m\r\n",
      "    \u001b[37m# s3.Bucket(bucket).download_file(key, fn)\u001b[39;49;00m\r\n",
      "    \r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mReading downloaded data.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    df = pd.read_csv(fn)\r\n",
      "    os.unlink(fn)\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mParsing dates as Datetime, and cyclically encoding day and month\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mDate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]= pd.to_datetime(df[\u001b[33m\"\u001b[39;49;00m\u001b[33mDate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33myear\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m\"\u001b[39;49;00m\u001b[33mDate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].dt.year\r\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mmonth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m\"\u001b[39;49;00m\u001b[33mDate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].dt.month\r\n",
      "    df = cyclical_encode(df, \u001b[33m'\u001b[39;49;00m\u001b[33mmonth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m12\u001b[39;49;00m)\r\n",
      "\r\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mday\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m\"\u001b[39;49;00m\u001b[33mDate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].dt.day\r\n",
      "    df = cyclical_encode(df, \u001b[33m'\u001b[39;49;00m\u001b[33mday\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m31\u001b[39;49;00m)\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mHandle missing categorical data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Get list of categorical data\u001b[39;49;00m\r\n",
      "    s = (df.dtypes == \u001b[33m\"\u001b[39;49;00m\u001b[33mobject\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    object_cols = \u001b[36mlist\u001b[39;49;00m(s[s].index)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m object_cols:\r\n",
      "        \u001b[37m# Fill missing data with mode\u001b[39;49;00m\r\n",
      "        df[i].fillna(   df[i].mode()[\u001b[34m0\u001b[39;49;00m], inplace=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mHandle missing numerical data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Get list of numerical data\u001b[39;49;00m\r\n",
      "    t = (df.dtypes == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat64\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    num_cols = \u001b[36mlist\u001b[39;49;00m(t[t].index)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m num_cols:\r\n",
      "        \u001b[37m# Fill in missing data w/ median\u001b[39;49;00m\r\n",
      "        df[i].fillna(df[i].median(), inplace=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mEncode Categorical Data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    label_encoder = LabelEncoder()\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m object_cols:\r\n",
      "        df[i] = label_encoder.fit_transform(df[i])\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mDropping unnecessary columns, scaling and separating target\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    features = df.drop([\u001b[33m'\u001b[39;49;00m\u001b[33mRainTomorrow\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mday\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmonth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m) \u001b[37m# dropping target and extra columns\u001b[39;49;00m\r\n",
      "\r\n",
      "    target = df[\u001b[33m'\u001b[39;49;00m\u001b[33mRainTomorrow\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[37m#Set up a standard scaler for the features\u001b[39;49;00m\r\n",
      "    col_names = \u001b[36mlist\u001b[39;49;00m(features.columns)\r\n",
      "    s_scaler = StandardScaler()\r\n",
      "    features = s_scaler.fit_transform(features)\r\n",
      "    features = pd.DataFrame(features, columns=col_names) \r\n",
      "\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mDrop outliers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# reinsert target after scaling\u001b[39;49;00m\r\n",
      "    features[\u001b[33m\"\u001b[39;49;00m\u001b[33mRainTomorrow\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = target\r\n",
      "\r\n",
      "    \u001b[37m#Dropping with outlier\u001b[39;49;00m\r\n",
      "\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mMinTemp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.3\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mMinTemp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2.3\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mMaxTemp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.3\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mMaxTemp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mRainfall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m4.5\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mEvaporation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.8\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mSunshine\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.1\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mWindGustSpeed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m4\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mWindGustSpeed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m4\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mWindSpeed9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m4\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mWindSpeed3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.5\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mHumidity9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m3\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mHumidity3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2.2\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mPressure9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]< \u001b[34m2\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mPressure9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2.7\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mPressure3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]< \u001b[34m2\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mPressure3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2.7\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mCloud9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m1.8\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mCloud3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mTemp9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.3\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mTemp9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mTemp3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.3\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mTemp3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2\u001b[39;49;00m)]\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mSplitting data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[37m# Notebook code\u001b[39;49;00m\r\n",
      "    X = features.drop([\u001b[33m\"\u001b[39;49;00m\u001b[33mRainTomorrow\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "    y = features[\u001b[33m\"\u001b[39;49;00m\u001b[33mRainTomorrow\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[37m# Splitting test and training sets\u001b[39;49;00m\r\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = \u001b[34m0.3\u001b[39;49;00m)\r\n",
      "    X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size = \u001b[34m0.5\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# concatenate fo caching\u001b[39;49;00m\r\n",
      "    train = pd.concat([y_train, X_train], axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "    validation = pd.concat([y_dev, X_dev], axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "    test = pd.concat([y_test, X_test], axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# SM template code\u001b[39;49;00m\r\n",
      "    \u001b[37m# logger.info(\"Applying transforms.\")\u001b[39;49;00m\r\n",
      "    \u001b[37m# y = df.pop(\"rings\")\u001b[39;49;00m\r\n",
      "    \u001b[37m# X_pre = preprocess.fit_transform(df)\u001b[39;49;00m\r\n",
      "    \u001b[37m# y_pre = y.to_numpy().reshape(len(y), 1)\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# X = np.concatenate((y_pre, X_pre), axis=1)\u001b[39;49;00m\r\n",
      "    \u001b[37m# X = features.to_numpy()\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# logger.info(\"Splitting %d rows of data into train, validation, test datasets.\", len(X))\u001b[39;49;00m\r\n",
      "    \u001b[37m# np.random.shuffle(features)\u001b[39;49;00m\r\n",
      "    \u001b[37m# train, validation, test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])\u001b[39;49;00m\r\n",
      "\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mWriting out datasets to \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, base_dir)\r\n",
      "    train.to_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/train/train.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, header=\u001b[34mTrue\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "    validation.to_csv(\r\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/validation/validation.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, header=\u001b[34mTrue\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    test.to_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/test/test.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, header=\u001b[34mTrue\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "    pathlib.Path(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/preprocess\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).mkdir(parents=\u001b[34mTrue\u001b[39;49;00m, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/preprocess/scaler.pkl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m dump_var:\r\n",
      "        pickle.dump(s_scaler, dump_var)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize pipelines/rain/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b4853b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  rainau-notebook-preprocess-2022-01-22-10-45-38-705\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://rain-data-17012022/data/weatherAUS.csv', 'LocalPath': '/opt/ml/processing/input/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/rainau-notebook-preprocess-2022-01-22-10-45-38-705/input/code/preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/rainau-notebook-preprocess-2022-01-22-10-45-38-705/output/train', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'validation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/rainau-notebook-preprocess-2022-01-22-10-45-38-705/output/validation', 'LocalPath': '/opt/ml/processing/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/rainau-notebook-preprocess-2022-01-22-10-45-38-705/output/test', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'scaler', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/rainau-notebook-preprocess-2022-01-22-10-45-38-705/output/scaler', 'LocalPath': '/opt/ml/processing/preprocess', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".......................\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\u001b[0m\n",
      "\u001b[34mWriting out datasets to /opt/ml/processing.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version='0.20.0',\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    volume_size_in_gb=15,\n",
    "    base_job_name=f\"rainau-notebook-preprocess\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "sklearn_processor.run(code=os.path.join(BASE_DIR, \"preprocess.py\"),\n",
    "    inputs=[ProcessingInput(source=input_data_source,\n",
    "        destination='/opt/ml/processing/input/')],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\",\n",
    "            source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\",\n",
    "            source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\",\n",
    "            source=\"/opt/ml/processing/test\"),\n",
    "        ProcessingOutput(output_name=\"scaler\",\n",
    "            source=\"/opt/ml/processing/preprocess\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fb58e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"s3://{sagemaker_session.default_bucket()}/{base_job_prefix}/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8f40724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-west-3-543553163241/RainAu/model'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e569bc0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-eu-west-3-543553163241/rainau-notebook-preprocess-2022-01-22-10-45-38-705/output/train', 'validation': 's3://sagemaker-eu-west-3-543553163241/rainau-notebook-preprocess-2022-01-22-10-45-38-705/output/validation', 'test': 's3://sagemaker-eu-west-3-543553163241/rainau-notebook-preprocess-2022-01-22-10-45-38-705/output/test', 'scaler': 's3://sagemaker-eu-west-3-543553163241/rainau-notebook-preprocess-2022-01-22-10-45-38-705/output/scaler'}\n"
     ]
    }
   ],
   "source": [
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "output_config = preprocessing_job_description[\"ProcessingOutputConfig\"]\n",
    "processing_output_uris = {}\n",
    "for output in output_config[\"Outputs\"]:\n",
    "    processing_output_uris[output[\"OutputName\"]] = output[\"S3Output\"][\"S3Uri\"]\n",
    "print(processing_output_uris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aa43a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af1fece",
   "metadata": {},
   "source": [
    "# Train the model in a Pytorch container\n",
    "\n",
    "The script `train.py` takes care of the training and saving of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8e6df885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/gmguarino/sagemaker-pipeline-rain-australia-build\r\n",
      " * branch            main       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f17054e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m#import sagemaker_containers\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dataset\r\n",
      "\r\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\r\n",
      "logger.setLevel(logging.INFO)\r\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m# Dataset class to handle data loading\u001b[39;49;00m\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mRainDataset\u001b[39;49;00m(Dataset):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, csv_file, root_dir):\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data = pd.read_csv(os.path.join(root_dir, csv_file))\r\n",
      "        \u001b[36mself\u001b[39;49;00m.labels = np.asarray(\u001b[36mself\u001b[39;49;00m.data.iloc[:, \u001b[34m0\u001b[39;49;00m])\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.labels)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, idx):\r\n",
      "        \u001b[34mif\u001b[39;49;00m torch.is_tensor(idx):\r\n",
      "            idx = idx.tolist()\r\n",
      "\r\n",
      "        single_label = \u001b[36mself\u001b[39;49;00m.labels[idx]\r\n",
      "\r\n",
      "        data_array = np.asarray(\u001b[36mself\u001b[39;49;00m.data.iloc[idx, \u001b[34m1\u001b[39;49;00m:])\r\n",
      "        feature_tensor = torch.tensor(data_array, dtype=torch.float32)\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m (feature_tensor, np.float32(single_label))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m#  Simple ANN for binary classification\u001b[39;49;00m\r\n",
      "\u001b[37m# Who cares this is a demo for sagemaker not a DL demo\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc1 = nn.Linear(\u001b[34m26\u001b[39;49;00m, \u001b[34m32\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc2 = nn.Linear(\u001b[34m32\u001b[39;49;00m, \u001b[34m32\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc3 = nn.Linear(\u001b[34m32\u001b[39;49;00m, \u001b[34m16\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc4 = nn.Linear(\u001b[34m16\u001b[39;49;00m, \u001b[34m8\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.out = nn.Linear(\u001b[34m8\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc1(x))\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc2(x))\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc3(x))\r\n",
      "        x = F.dropout(x, p=\u001b[34m0.25\u001b[39;49;00m, training=\u001b[36mself\u001b[39;49;00m.training)\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc4(x))\r\n",
      "        x = F.dropout(x, p=\u001b[34m0.5\u001b[39;49;00m, training=\u001b[36mself\u001b[39;49;00m.training)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m F.sigmoid(\u001b[36mself\u001b[39;49;00m.out(x))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(batch_size, training_dir, is_distributed, **kwargs):\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    dataset = RainDataset(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        training_dir\r\n",
      "    )\r\n",
      "    train_sampler = (\r\n",
      "        torch.utils.data.distributed.DistributedSampler(dataset) \u001b[34mif\u001b[39;49;00m is_distributed \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(\r\n",
      "        dataset,\r\n",
      "        batch_size=batch_size,\r\n",
      "        shuffle=train_sampler \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\r\n",
      "        sampler=train_sampler,\r\n",
      "        **kwargs\r\n",
      "    )\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_average_gradients\u001b[39;49;00m(model):\r\n",
      "    \u001b[37m# Gradient averaging.\u001b[39;49;00m\r\n",
      "    size = \u001b[36mfloat\u001b[39;49;00m(dist.get_world_size())\r\n",
      "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\r\n",
      "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\r\n",
      "        param.grad.data /= size\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmake_tarfile\u001b[39;49;00m(output_filename, source_dir):\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tarfile.open(output_filename, \u001b[33m\"\u001b[39;49;00m\u001b[33mw:gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m tar:\r\n",
      "        \u001b[34mfor\u001b[39;49;00m filename \u001b[35min\u001b[39;49;00m os.listdir(source_dir):\r\n",
      "            tar.add(os.path.join(source_dir, filename), arcname=filename)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\r\n",
      "    is_distributed = \u001b[36mlen\u001b[39;49;00m(args.hosts) > \u001b[34m1\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m args.backend \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mDistributed training - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(is_distributed))\r\n",
      "    use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of gpus available - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.num_gpus))\r\n",
      "    kwargs = {\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m} \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m {}\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed:\r\n",
      "        \u001b[37m# Initialize the distributed environment.\u001b[39;49;00m\r\n",
      "        world_size = \u001b[36mlen\u001b[39;49;00m(args.hosts)\r\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mWORLD_SIZE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(world_size)\r\n",
      "        host_rank = args.hosts.index(args.current_host)\r\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mRANK\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(host_rank)\r\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\r\n",
      "        logger.info(\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mInitialized the distributed environment: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m backend on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m nodes. \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "                args.backend, dist.get_world_size()\r\n",
      "            )\r\n",
      "            + \u001b[33m\"\u001b[39;49;00m\u001b[33mCurrent host rank is \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m. Number of gpus: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(dist.get_rank(), args.num_gpus)\r\n",
      "        )\r\n",
      "\r\n",
      "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\r\n",
      "    torch.manual_seed(args.seed)\r\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\r\n",
      "        torch.cuda.manual_seed(args.seed)\r\n",
      "\r\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed, **kwargs)\r\n",
      "\r\n",
      "    logger.debug(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "            \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\r\n",
      "            \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\r\n",
      "            \u001b[34m100.0\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(train_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\r\n",
      "        )\r\n",
      "    )\r\n",
      "\r\n",
      "\r\n",
      "    model = Net().to(device)\r\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m use_cuda:\r\n",
      "        \u001b[37m# multi-machine multi-gpu case\u001b[39;49;00m\r\n",
      "        model = torch.nn.parallel.DistributedDataParallel(model)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[37m# single-machine multi-gpu case or single-machine or multi-machine cpu case\u001b[39;49;00m\r\n",
      "        model = torch.nn.DataParallel(model)\r\n",
      "\r\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\r\n",
      "\r\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\r\n",
      "        model.train()\r\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\r\n",
      "            data, target = data.to(device), target.to(device)\r\n",
      "            optimizer.zero_grad()\r\n",
      "            output = model(data)\r\n",
      "            loss = F.binary_cross_entropy(output.squeeze(), target.squeeze())\r\n",
      "            loss.backward()\r\n",
      "            \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m use_cuda:\r\n",
      "                \u001b[37m# average gradients manually for multi-machine cpu case only\u001b[39;49;00m\r\n",
      "                _average_gradients(model)\r\n",
      "            optimizer.step()\r\n",
      "            \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m:\r\n",
      "                logger.info(\r\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "                        epoch,\r\n",
      "                        batch_idx * \u001b[36mlen\u001b[39;49;00m(data),\r\n",
      "                        \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\r\n",
      "                        \u001b[34m100.0\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader),\r\n",
      "                        loss.item(),\r\n",
      "                    )\r\n",
      "                )\r\n",
      "    save_model(model, args.model_dir)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model = torch.nn.DataParallel(Net())\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\r\n",
      "    \u001b[37m# Model is saved both as a deployment torchscript as well as a normal pth file\u001b[39;49;00m\r\n",
      "    \u001b[37m# for evaluation in the pipeline\u001b[39;49;00m\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    path = os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    torch.save(model.cpu().state_dict(), path)\r\n",
      "    \u001b[37m# inference_code_path = model_dir + \"/code/\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# # if not os.path.exists(inference_code_path):\u001b[39;49;00m\r\n",
      "    \u001b[37m# #     os.mkdir(inference_code_path)\u001b[39;49;00m\r\n",
      "    \u001b[37m# #     logger.info(\"Created a folder at {}!\".format(inference_code_path))\u001b[39;49;00m\r\n",
      "    \u001b[37m# pathlib.Path(inference_code_path).mkdir(parents=True, exist_ok=True)\u001b[39;49;00m\r\n",
      "    \u001b[37m# base_dir = os.path.dirname(os.path.realpath(__file__))\u001b[39;49;00m\r\n",
      "    \u001b[37m# shutil.copy(os.path.join(base_dir,\"inference.py\"), inference_code_path)\u001b[39;49;00m\r\n",
      "    \u001b[37m# make_tarfile(os.path.join(model_dir, \"model.tar.gz\"), model_dir)\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m10\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.00009\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 0.00009)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m100\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "\r\n",
      "    train(parser.parse_args())\r\n",
      "\r\n",
      "    \u001b[37m# TODO: REMOVE JIT LEAVE AS NORMAL MODEL\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize pipelines/rain/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c85650e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-22 17:59:09 Starting - Starting the training job...\n",
      "2022-01-22 17:59:11 Starting - Launching requested ML instancesProfilerReport-1642874349: InProgress\n",
      "...\n",
      "2022-01-22 18:00:01 Starting - Preparing the instances for training.........\n",
      "2022-01-22 18:01:33 Downloading - Downloading input data...\n",
      "2022-01-22 18:01:54 Training - Downloading the training image.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-01-22 18:02:12,745 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-01-22 18:02:12,747 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-22 18:02:12,757 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-01-22 18:02:15,775 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-01-22 18:02:16,103 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-22 18:02:16,115 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-22 18:02:16,126 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-22 18:02:16,137 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 32,\n",
      "        \"lr\": 9e-05,\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"RainAu-torch-train-notebook-2022-01-22-17-59-09-214\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-3-543553163241/RainAu-torch-train-notebook-2022-01-22-17-59-09-214/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":32,\"epochs\":1,\"lr\":9e-05}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-3-543553163241/RainAu-torch-train-notebook-2022-01-22-17-59-09-214/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":32,\"epochs\":1,\"lr\":9e-05},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"RainAu-torch-train-notebook-2022-01-22-17-59-09-214\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-3-543553163241/RainAu-torch-train-notebook-2022-01-22-17-59-09-214/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"32\",\"--epochs\",\"1\",\"--lr\",\"9e-05\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_LR=9e-05\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --batch-size 32 --epochs 1 --lr 9e-05\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.302 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.552 algo-1:27 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.552 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.553 algo-1:27 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.553 algo-1:27 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.553 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.578 algo-1:27 INFO hook.py:584] name:module.fc1.weight count_params:832\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.578 algo-1:27 INFO hook.py:584] name:module.fc1.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.578 algo-1:27 INFO hook.py:584] name:module.fc2.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.578 algo-1:27 INFO hook.py:584] name:module.fc2.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.578 algo-1:27 INFO hook.py:584] name:module.fc3.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.578 algo-1:27 INFO hook.py:584] name:module.fc3.bias count_params:16\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.578 algo-1:27 INFO hook.py:584] name:module.fc4.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.578 algo-1:27 INFO hook.py:584] name:module.fc4.bias count_params:8\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.578 algo-1:27 INFO hook.py:584] name:module.out.weight count_params:8\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.578 algo-1:27 INFO hook.py:584] name:module.out.bias count_params:1\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.578 algo-1:27 INFO hook.py:586] Total Trainable Params: 2593\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.579 algo-1:27 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:02:18.581 algo-1:27 INFO hook.py:476] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3200/89275 (4%)] Loss: 0.712374\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/89275 (7%)] Loss: 0.694392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [9600/89275 (11%)] Loss: 0.664840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/89275 (14%)] Loss: 0.637017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [16000/89275 (18%)] Loss: 0.645708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/89275 (22%)] Loss: 0.645639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [22400/89275 (25%)] Loss: 0.633609\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/89275 (29%)] Loss: 0.531594\u001b[0m\n",
      "\n",
      "2022-01-22 18:02:32 Training - Training image download completed. Training in progress.\u001b[34mTrain Epoch: 1 [28800/89275 (32%)] Loss: 0.588786\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/89275 (36%)] Loss: 0.578363\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [35200/89275 (39%)] Loss: 0.643165\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/89275 (43%)] Loss: 0.425520\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [41600/89275 (47%)] Loss: 0.432609\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/89275 (50%)] Loss: 0.520785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [48000/89275 (54%)] Loss: 0.559173\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/89275 (57%)] Loss: 0.444274\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [54400/89275 (61%)] Loss: 0.459326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/89275 (65%)] Loss: 0.333052\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTrain Epoch: 1 [60800/89275 (68%)] Loss: 0.561671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [64000/89275 (72%)] Loss: 0.621409\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [67200/89275 (75%)] Loss: 0.432216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [70400/89275 (79%)] Loss: 0.330699\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [73600/89275 (82%)] Loss: 0.453471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [76800/89275 (86%)] Loss: 0.448097\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [80000/89275 (90%)] Loss: 0.439875\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [83200/89275 (93%)] Loss: 0.371524\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [86400/89275 (97%)] Loss: 0.578737\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3200/89275 (4%)] Loss: 0.712374\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/89275 (7%)] Loss: 0.694392\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [9600/89275 (11%)] Loss: 0.664840\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/89275 (14%)] Loss: 0.637017\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [16000/89275 (18%)] Loss: 0.645708\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/89275 (22%)] Loss: 0.645639\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [22400/89275 (25%)] Loss: 0.633609\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/89275 (29%)] Loss: 0.531594\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [28800/89275 (32%)] Loss: 0.588786\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/89275 (36%)] Loss: 0.578363\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [35200/89275 (39%)] Loss: 0.643165\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/89275 (43%)] Loss: 0.425520\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [41600/89275 (47%)] Loss: 0.432609\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/89275 (50%)] Loss: 0.520785\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [48000/89275 (54%)] Loss: 0.559173\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/89275 (57%)] Loss: 0.444274\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [54400/89275 (61%)] Loss: 0.459326\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/89275 (65%)] Loss: 0.333052\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [60800/89275 (68%)] Loss: 0.561671\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [64000/89275 (72%)] Loss: 0.621409\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [67200/89275 (75%)] Loss: 0.432216\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [70400/89275 (79%)] Loss: 0.330699\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [73600/89275 (82%)] Loss: 0.453471\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [76800/89275 (86%)] Loss: 0.448097\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [80000/89275 (90%)] Loss: 0.439875\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [83200/89275 (93%)] Loss: 0.371524\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [86400/89275 (97%)] Loss: 0.578737\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34m2022-01-22 18:02:44,829 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-01-22 18:02:56 Uploading - Uploading generated training model\n",
      "2022-01-22 18:02:56 Completed - Training job completed\n",
      "Training seconds: 83\n",
      "Billable seconds: 83\n"
     ]
    }
   ],
   "source": [
    "pytorch_estimator = PyTorch(os.path.join(BASE_DIR, 'train.py'),\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version='1.8.0',\n",
    "    py_version='py3',\n",
    "    base_job_name=f\"{base_job_prefix}-torch-train-notebook\",\n",
    "    output_path=model_path,\n",
    "    hyperparameters={'epochs': 1, 'batch-size': 32, 'lr': 0.00009},\n",
    "    role=role)\n",
    "pytorch_estimator.fit({'train': processing_output_uris[\"train\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "45ccfb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-west-3-543553163241/RainAu/model/RainAu-torch-train-notebook-2022-01-22-17-59-09-214/output/model.tar.gz'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5aea5",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "303988d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/gmguarino/sagemaker-pipeline-rain-australia-build\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6fa9c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.workflow.properties import PropertyFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cb5c3310",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  RainAu-torch-eval-2022-01-22-18-03-22-287\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/RainAu/model/RainAu-torch-train-notebook-2022-01-22-17-59-09-214/output/model.tar.gz', 'LocalPath': '/opt/ml/processing/model', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/rainau-notebook-preprocess-2022-01-22-10-45-38-705/output/validation', 'LocalPath': '/opt/ml/processing/validation', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/RainAu-torch-eval-2022-01-22-18-03-22-287/source/sourcedir.tar.gz', 'LocalPath': '/opt/ml/processing/input/code/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'entrypoint', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/RainAu-torch-eval-2022-01-22-18-03-22-287/source/runproc.sh', 'LocalPath': '/opt/ml/processing/input/entrypoint', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'evaluation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/RainAu-torch-eval-2022-01-22-18-03-22-287/output/evaluation', 'LocalPath': '/opt/ml/processing/evaluation', 'S3UploadMode': 'EndOfJob'}}]\n",
      "..........................\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:07:29.727 ip-10-0-181-112.eu-west-3.compute.internal:9 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-01-22 18:07:29.789 ip-10-0-181-112.eu-west-3.compute.internal:9 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34mTest set stats: \n",
      " Num examples: {'accuracy': 0.8328653846153845, 'precision': 0.6509762873908326, 'recall': 0.4358036483979058, 'f1_score': 0.5214194391645994},  \n",
      " {'accuracy': 0.8328653846153845, 'precision': 0.6509762873908326, 'recall': 0.4358036483979058, 'f1_score': 0.5214194391645994}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pytorch_processor = FrameworkProcessor(\n",
    "    PyTorch,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version='1.8.0',\n",
    "    base_job_name=f\"{base_job_prefix}-torch-eval\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    "    command=[\"python3\"],\n",
    ")\n",
    "\n",
    "pytorch_processor.run(code=os.path.join(BASE_DIR, \"evaluate.py\"),\n",
    "    inputs=[ProcessingInput(source=pytorch_estimator.model_data, destination=\"/opt/ml/processing/model\"), \n",
    "        # ProcessingInput(source=model_path,destination=\"/opt/ml/processing/model\"), \n",
    "        ProcessingInput(source=processing_output_uris[\"validation\"],\n",
    "            destination=\"/opt/ml/processing/validation\")\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(output_name=\"evaluation\",source=\"/opt/ml/processing/evaluation\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "27795b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evaluation': 's3://sagemaker-eu-west-3-543553163241/RainAu-torch-eval-2022-01-22-18-03-22-287/output/evaluation'}\n"
     ]
    }
   ],
   "source": [
    "evaluation_job_description = pytorch_processor.jobs[-1].describe()\n",
    "output_config = evaluation_job_description[\"ProcessingOutputConfig\"]\n",
    "evaluation_output_uris = {}\n",
    "for output in output_config[\"Outputs\"]:\n",
    "    evaluation_output_uris[output[\"OutputName\"]] = output[\"S3Output\"][\"S3Uri\"]\n",
    "print(evaluation_output_uris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1ee8721f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regression_metrics': {'accuracy': 0.8328653846153845, 'precision': 0.6509762873908326, 'recall': 0.4358036483979058, 'f1_score': 0.5214194391645994}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "uri = evaluation_output_uris[\"evaluation\"]+ \"/evaluation.json\"\n",
    "\n",
    "obj = s3_client.get_object(Bucket=uri.split(\"/\")[2], Key=\"/\".join(uri.split(\"/\")[3:]))\n",
    "\n",
    "report = json.loads(obj['Body'].read())\n",
    "#df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "\n",
    "\n",
    "#with open(evaluation_output_uris[\"evaluation\"]+ \"/evaluation.json\") as jf:\n",
    "#    report = json.load(jf)\n",
    "    \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af186920",
   "metadata": {},
   "source": [
    "Here we have only trained the model for one epoch (demo purposes), so the performance is not great. Let's say though that we are satisfied, we can then decide to deploy our model to an endpoint. Let's start by loading in a Pytorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a40921d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 9, done.\u001b[K\r\n",
      "remote: Counting objects:  11% (1/9)\u001b[K\r",
      "remote: Counting objects:  22% (2/9)\u001b[K\r",
      "remote: Counting objects:  33% (3/9)\u001b[K\r",
      "remote: Counting objects:  44% (4/9)\u001b[K\r",
      "remote: Counting objects:  55% (5/9)\u001b[K\r",
      "remote: Counting objects:  66% (6/9)\u001b[K\r",
      "remote: Counting objects:  77% (7/9)\u001b[K\r",
      "remote: Counting objects:  88% (8/9)\u001b[K\r",
      "remote: Counting objects: 100% (9/9)\u001b[K\r",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (1/1)\u001b[K\r",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\r\n",
      "remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0\u001b[K\r\n",
      "Unpacking objects:  20% (1/5)\r",
      "Unpacking objects:  40% (2/5)\r",
      "Unpacking objects:  60% (3/5)\r",
      "Unpacking objects:  80% (4/5)\r",
      "Unpacking objects: 100% (5/5)\r",
      "Unpacking objects: 100% (5/5), 494 bytes | 247.00 KiB/s, done.\r\n",
      "From https://github.com/gmguarino/sagemaker-pipeline-rain-australia-build\r\n",
      " * branch            main       -> FETCH_HEAD\r\n",
      "   e9a4a7b..dff7c1d  main       -> origin/main\r\n",
      "Updating e9a4a7b..dff7c1d\r\n",
      "Fast-forward\r\n",
      " pipelines/rain/inference.py | 35 \u001b[32m++++++++++++++++++\u001b[m\u001b[31m-----------------\u001b[m\r\n",
      " 1 file changed, 18 insertions(+), 17 deletions(-)\r\n"
     ]
    }
   ],
   "source": [
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ead86045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "88a097c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model_deployment = PyTorchModel(\n",
    "    entry_point='inference.py',\n",
    "    source_dir=BASE_DIR,\n",
    "    model_data=pytorch_estimator.model_data,\n",
    "    role=role,\n",
    "    framework_version='1.8.0',\n",
    "    py_version='py3',\n",
    "    name=\"rain-au-deployment-model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "30c5d827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PyTorchModel in module sagemaker.pytorch.model:\n",
      "\n",
      "class PyTorchModel(sagemaker.model.FrameworkModel)\n",
      " |  PyTorchModel(model_data, role, entry_point, framework_version=None, py_version=None, image_uri=None, predictor_cls=<class 'sagemaker.pytorch.model.PyTorchPredictor'>, model_server_workers=None, **kwargs)\n",
      " |  \n",
      " |  An PyTorch SageMaker ``Model`` that can be deployed to a SageMaker ``Endpoint``.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PyTorchModel\n",
      " |      sagemaker.model.FrameworkModel\n",
      " |      sagemaker.model.Model\n",
      " |      sagemaker.model.ModelBase\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model_data, role, entry_point, framework_version=None, py_version=None, image_uri=None, predictor_cls=<class 'sagemaker.pytorch.model.PyTorchPredictor'>, model_server_workers=None, **kwargs)\n",
      " |      Initialize a PyTorchModel.\n",
      " |      \n",
      " |      Args:\n",
      " |          model_data (str): The S3 location of a SageMaker model data\n",
      " |              ``.tar.gz`` file.\n",
      " |          role (str): An AWS IAM role (either name or full ARN). The Amazon\n",
      " |              SageMaker training jobs and APIs that create Amazon SageMaker\n",
      " |              endpoints use this role to access training data and model\n",
      " |              artifacts. After the endpoint is created, the inference code\n",
      " |              might use the IAM role, if it needs to access an AWS resource.\n",
      " |          entry_point (str): Path (absolute or relative) to the Python source\n",
      " |              file which should be executed as the entry point to model\n",
      " |              hosting. If ``source_dir`` is specified, then ``entry_point``\n",
      " |              must point to a file located at the root of ``source_dir``.\n",
      " |          framework_version (str): PyTorch version you want to use for\n",
      " |              executing your model training code. Defaults to None. Required\n",
      " |              unless ``image_uri`` is provided.\n",
      " |          py_version (str): Python version you want to use for executing your\n",
      " |              model training code. Defaults to ``None``. Required unless\n",
      " |              ``image_uri`` is provided.\n",
      " |          image_uri (str): A Docker image URI (default: None). If not specified, a\n",
      " |              default image for PyTorch will be used. If ``framework_version``\n",
      " |              or ``py_version`` are ``None``, then ``image_uri`` is required. If\n",
      " |              also ``None``, then a ``ValueError`` will be raised.\n",
      " |          predictor_cls (callable[str, sagemaker.session.Session]): A function\n",
      " |              to call to create a predictor with an endpoint name and\n",
      " |              SageMaker ``Session``. If specified, ``deploy()`` returns the\n",
      " |              result of invoking this function on the created endpoint name.\n",
      " |          model_server_workers (int): Optional. The number of worker processes\n",
      " |              used by the inference server. If None, server will use one\n",
      " |              worker per vCPU.\n",
      " |          **kwargs: Keyword arguments passed to the superclass\n",
      " |              :class:`~sagemaker.model.FrameworkModel` and, subsequently, its\n",
      " |              superclass :class:`~sagemaker.model.Model`.\n",
      " |      \n",
      " |      .. tip::\n",
      " |      \n",
      " |          You can find additional parameters for initializing this class at\n",
      " |          :class:`~sagemaker.model.FrameworkModel` and\n",
      " |          :class:`~sagemaker.model.Model`.\n",
      " |  \n",
      " |  prepare_container_def(self, instance_type=None, accelerator_type=None)\n",
      " |      A container definition with framework configuration set in model environment variables.\n",
      " |      \n",
      " |      Args:\n",
      " |          instance_type (str): The EC2 instance type to deploy this Model to.\n",
      " |              For example, 'ml.p2.xlarge'.\n",
      " |          accelerator_type (str): The Elastic Inference accelerator type to\n",
      " |              deploy to the instance for loading and making inferences to the\n",
      " |              model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict[str, str]: A container definition object usable with the\n",
      " |          CreateModel API.\n",
      " |  \n",
      " |  register(self, content_types, response_types, inference_instances, transform_instances, model_package_name=None, model_package_group_name=None, image_uri=None, model_metrics=None, metadata_properties=None, marketplace_cert=False, approval_status=None, description=None, drift_check_baselines=None)\n",
      " |      Creates a model package for creating SageMaker models or listing on Marketplace.\n",
      " |      \n",
      " |      Args:\n",
      " |          content_types (list): The supported MIME types for the input data.\n",
      " |          response_types (list): The supported MIME types for the output data.\n",
      " |          inference_instances (list): A list of the instance types that are used to\n",
      " |              generate inferences in real-time.\n",
      " |          transform_instances (list): A list of the instance types on which a transformation\n",
      " |              job can be run or on which an endpoint can be deployed.\n",
      " |          model_package_name (str): Model Package name, exclusive to `model_package_group_name`,\n",
      " |              using `model_package_name` makes the Model Package un-versioned (default: None).\n",
      " |          model_package_group_name (str): Model Package Group name, exclusive to\n",
      " |              `model_package_name`, using `model_package_group_name` makes the Model Package\n",
      " |              versioned (default: None).\n",
      " |          image_uri (str): Inference image uri for the container. Model class' self.image will\n",
      " |              be used if it is None (default: None).\n",
      " |          model_metrics (ModelMetrics): ModelMetrics object (default: None).\n",
      " |          metadata_properties (MetadataProperties): MetadataProperties object (default: None).\n",
      " |          marketplace_cert (bool): A boolean value indicating if the Model Package is certified\n",
      " |              for AWS Marketplace (default: False).\n",
      " |          approval_status (str): Model Approval Status, values can be \"Approved\", \"Rejected\",\n",
      " |              or \"PendingManualApproval\" (default: \"PendingManualApproval\").\n",
      " |          description (str): Model Package description (default: None).\n",
      " |          drift_check_baselines (DriftCheckBaselines): DriftCheckBaselines object (default: None).\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `sagemaker.model.ModelPackage` instance.\n",
      " |  \n",
      " |  serving_image_uri(self, region_name, instance_type, accelerator_type=None)\n",
      " |      Create a URI for the serving image.\n",
      " |      \n",
      " |      Args:\n",
      " |          region_name (str): AWS region where the image is uploaded.\n",
      " |          instance_type (str): SageMaker instance type. Used to determine device type\n",
      " |              (cpu/gpu/family-specific optimized).\n",
      " |          accelerator_type (str): The Elastic Inference accelerator type to\n",
      " |              deploy to the instance for loading and making inferences to the\n",
      " |              model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: The appropriate image URI based on the given parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sagemaker.model.Model:\n",
      " |  \n",
      " |  compile(self, target_instance_family, input_shape, output_path, role, tags=None, job_name=None, compile_max_run=900, framework=None, framework_version=None, target_platform_os=None, target_platform_arch=None, target_platform_accelerator=None, compiler_options=None)\n",
      " |      Compile this ``Model`` with SageMaker Neo.\n",
      " |      \n",
      " |      Args:\n",
      " |          target_instance_family (str): Identifies the device that you want to\n",
      " |              run your model after compilation, for example: ml_c5. For allowed\n",
      " |              strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |              Alternatively, you can select an OS, Architecture and Accelerator using\n",
      " |              ``target_platform_os``, ``target_platform_arch``,\n",
      " |              and ``target_platform_accelerator``.\n",
      " |          input_shape (dict): Specifies the name and shape of the expected\n",
      " |              inputs for your trained model in json dictionary form, for\n",
      " |              example: {'data': [1,3,1024,1024]}, or {'var1': [1,1,28,28],\n",
      " |              'var2': [1,1,28,28]}\n",
      " |          output_path (str): Specifies where to store the compiled model\n",
      " |          role (str): Execution role\n",
      " |          tags (list[dict]): List of tags for labeling a compilation job. For\n",
      " |              more, see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      " |          job_name (str): The name of the compilation job\n",
      " |          compile_max_run (int): Timeout in seconds for compilation (default:\n",
      " |              15 * 60). After this amount of time Amazon SageMaker Neo\n",
      " |              terminates the compilation job regardless of its current status.\n",
      " |          framework (str): The framework that is used to train the original\n",
      " |              model. Allowed values: 'mxnet', 'tensorflow', 'keras', 'pytorch',\n",
      " |              'onnx', 'xgboost'\n",
      " |          framework_version (str): The version of framework, for example:\n",
      " |              '1.5' for PyTorch\n",
      " |          target_platform_os (str): Target Platform OS, for example: 'LINUX'.\n",
      " |              For allowed strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |              It can be used instead of target_instance_family by setting target_instance\n",
      " |              family to None.\n",
      " |          target_platform_arch (str): Target Platform Architecture, for example: 'X86_64'.\n",
      " |              For allowed strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |              It can be used instead of target_instance_family by setting target_instance\n",
      " |              family to None.\n",
      " |          target_platform_accelerator (str, optional): Target Platform Accelerator,\n",
      " |              for example: 'NVIDIA'. For allowed strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |              It can be used instead of target_instance_family by setting target_instance\n",
      " |              family to None.\n",
      " |          compiler_options (dict, optional): Additional parameters for compiler.\n",
      " |              Compiler Options are TargetPlatform / target_instance_family specific. See\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html for details.\n",
      " |      \n",
      " |      Returns:\n",
      " |          sagemaker.model.Model: A SageMaker ``Model`` object. See\n",
      " |          :func:`~sagemaker.model.Model` for full details.\n",
      " |  \n",
      " |  delete_model(self)\n",
      " |      Delete an Amazon SageMaker Model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the model is not created yet.\n",
      " |  \n",
      " |  deploy(self, initial_instance_count, instance_type, serializer=None, deserializer=None, accelerator_type=None, endpoint_name=None, tags=None, kms_key=None, wait=True, data_capture_config=None, **kwargs)\n",
      " |      Deploy this ``Model`` to an ``Endpoint`` and optionally return a ``Predictor``.\n",
      " |      \n",
      " |      Create a SageMaker ``Model`` and ``EndpointConfig``, and deploy an\n",
      " |      ``Endpoint`` from this ``Model``. If ``self.predictor_cls`` is not None,\n",
      " |      this method returns a the result of invoking ``self.predictor_cls`` on\n",
      " |      the created endpoint name.\n",
      " |      \n",
      " |      The name of the created model is accessible in the ``name`` field of\n",
      " |      this ``Model`` after deploy returns\n",
      " |      \n",
      " |      The name of the created endpoint is accessible in the\n",
      " |      ``endpoint_name`` field of this ``Model`` after deploy returns.\n",
      " |      \n",
      " |      Args:\n",
      " |          initial_instance_count (int): The initial number of instances to run\n",
      " |              in the ``Endpoint`` created from this ``Model``.\n",
      " |          instance_type (str): The EC2 instance type to deploy this Model to.\n",
      " |              For example, 'ml.p2.xlarge', or 'local' for local mode.\n",
      " |          serializer (:class:`~sagemaker.serializers.BaseSerializer`): A\n",
      " |              serializer object, used to encode data for an inference endpoint\n",
      " |              (default: None). If ``serializer`` is not None, then\n",
      " |              ``serializer`` will override the default serializer. The\n",
      " |              default serializer is set by the ``predictor_cls``.\n",
      " |          deserializer (:class:`~sagemaker.deserializers.BaseDeserializer`): A\n",
      " |              deserializer object, used to decode data from an inference\n",
      " |              endpoint (default: None). If ``deserializer`` is not None, then\n",
      " |              ``deserializer`` will override the default deserializer. The\n",
      " |              default deserializer is set by the ``predictor_cls``.\n",
      " |          accelerator_type (str): Type of Elastic Inference accelerator to\n",
      " |              deploy this model for model loading and inference, for example,\n",
      " |              'ml.eia1.medium'. If not specified, no Elastic Inference\n",
      " |              accelerator will be attached to the endpoint. For more\n",
      " |              information:\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n",
      " |          endpoint_name (str): The name of the endpoint to create (default:\n",
      " |              None). If not specified, a unique endpoint name will be created.\n",
      " |          tags (List[dict[str, str]]): The list of tags to attach to this\n",
      " |              specific endpoint.\n",
      " |          kms_key (str): The ARN of the KMS key that is used to encrypt the\n",
      " |              data on the storage volume attached to the instance hosting the\n",
      " |              endpoint.\n",
      " |          wait (bool): Whether the call should wait until the deployment of\n",
      " |              this model completes (default: True).\n",
      " |          data_capture_config (sagemaker.model_monitor.DataCaptureConfig): Specifies\n",
      " |              configuration related to Endpoint data capture for use with\n",
      " |              Amazon SageMaker Model Monitoring. Default: None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          callable[string, sagemaker.session.Session] or None: Invocation of\n",
      " |              ``self.predictor_cls`` on the created endpoint name, if ``self.predictor_cls``\n",
      " |              is not None. Otherwise, return None.\n",
      " |  \n",
      " |  enable_network_isolation(self)\n",
      " |      Whether to enable network isolation when creating this Model\n",
      " |      \n",
      " |      Returns:\n",
      " |          bool: If network isolation should be enabled or not.\n",
      " |  \n",
      " |  package_for_edge(self, output_path, model_name, model_version, role=None, job_name=None, resource_key=None, s3_kms_key=None, tags=None)\n",
      " |      Package this ``Model`` with SageMaker Edge.\n",
      " |      \n",
      " |      Creates a new EdgePackagingJob and wait for it to finish.\n",
      " |      model_data will now point to the packaged artifacts.\n",
      " |      \n",
      " |      Args:\n",
      " |          output_path (str): Specifies where to store the packaged model\n",
      " |          role (str): Execution role\n",
      " |          model_name (str): the name to attach to the model metadata\n",
      " |          model_version (str): the version to attach to the model metadata\n",
      " |          job_name (str): The name of the edge packaging job\n",
      " |          resource_key (str): the kms key to encrypt the disk with\n",
      " |          s3_kms_key (str): the kms key to encrypt the output with\n",
      " |          tags (list[dict]): List of tags for labeling an edge packaging job. For\n",
      " |              more, see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      " |      \n",
      " |      Returns:\n",
      " |          sagemaker.model.Model: A SageMaker ``Model`` object. See\n",
      " |          :func:`~sagemaker.model.Model` for full details.\n",
      " |  \n",
      " |  transformer(self, instance_count, instance_type, strategy=None, assemble_with=None, output_path=None, output_kms_key=None, accept=None, env=None, max_concurrent_transforms=None, max_payload=None, tags=None, volume_kms_key=None)\n",
      " |      Return a ``Transformer`` that uses this Model.\n",
      " |      \n",
      " |      Args:\n",
      " |          instance_count (int): Number of EC2 instances to use.\n",
      " |          instance_type (str): Type of EC2 instance to use, for example,\n",
      " |              'ml.c4.xlarge'.\n",
      " |          strategy (str): The strategy used to decide how to batch records in\n",
      " |              a single request (default: None). Valid values: 'MultiRecord'\n",
      " |              and 'SingleRecord'.\n",
      " |          assemble_with (str): How the output is assembled (default: None).\n",
      " |              Valid values: 'Line' or 'None'.\n",
      " |          output_path (str): S3 location for saving the transform result. If\n",
      " |              not specified, results are stored to a default bucket.\n",
      " |          output_kms_key (str): Optional. KMS key ID for encrypting the\n",
      " |              transform output (default: None).\n",
      " |          accept (str): The accept header passed by the client to\n",
      " |              the inference endpoint. If it is supported by the endpoint,\n",
      " |              it will be the format of the batch transform output.\n",
      " |          env (dict): Environment variables to be set for use during the\n",
      " |              transform job (default: None).\n",
      " |          max_concurrent_transforms (int): The maximum number of HTTP requests\n",
      " |              to be made to each individual transform container at one time.\n",
      " |          max_payload (int): Maximum size of the payload in a single HTTP\n",
      " |              request to the container in MB.\n",
      " |          tags (list[dict]): List of tags for labeling a transform job. If\n",
      " |              none specified, then the tags used for the training job are used\n",
      " |              for the transform job.\n",
      " |          volume_kms_key (str): Optional. KMS key ID for encrypting the volume\n",
      " |              attached to the ML compute instance (default: None).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sagemaker.model.ModelBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(PyTorchModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "07000bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: rain-au-deployment-model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "predictor = pytorch_model_deployment.deploy(\n",
    "    initial_instance_count=inference_instance_count,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=\"pytorch-rain-au-model-test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6e22d65e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-eu-west-3-543553163241 rainau-notebook-preprocess-2022-01-22-10-45-38-705/output/test/test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RainTomorrow</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>year</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.928162</td>\n",
       "      <td>0.692446</td>\n",
       "      <td>-0.312621</td>\n",
       "      <td>-0.203581</td>\n",
       "      <td>1.264694</td>\n",
       "      <td>-2.063213</td>\n",
       "      <td>1.045228</td>\n",
       "      <td>-0.073333</td>\n",
       "      <td>0.885879</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464068</td>\n",
       "      <td>1.087018</td>\n",
       "      <td>-0.029011</td>\n",
       "      <td>-0.593947</td>\n",
       "      <td>-0.529795</td>\n",
       "      <td>-1.879575</td>\n",
       "      <td>-1.244369</td>\n",
       "      <td>0.728636</td>\n",
       "      <td>0.416432</td>\n",
       "      <td>-1.334442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.601947</td>\n",
       "      <td>0.205466</td>\n",
       "      <td>-0.707636</td>\n",
       "      <td>-0.275097</td>\n",
       "      <td>-0.371139</td>\n",
       "      <td>1.236542</td>\n",
       "      <td>1.045228</td>\n",
       "      <td>-0.073333</td>\n",
       "      <td>-1.107113</td>\n",
       "      <td>...</td>\n",
       "      <td>1.025756</td>\n",
       "      <td>-1.760956</td>\n",
       "      <td>-0.230584</td>\n",
       "      <td>-0.623142</td>\n",
       "      <td>-0.529795</td>\n",
       "      <td>-1.485514</td>\n",
       "      <td>-0.725379</td>\n",
       "      <td>1.245139</td>\n",
       "      <td>-0.803968</td>\n",
       "      <td>1.199371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.899139</td>\n",
       "      <td>-0.312933</td>\n",
       "      <td>-0.552452</td>\n",
       "      <td>-0.275097</td>\n",
       "      <td>-0.119472</td>\n",
       "      <td>0.148710</td>\n",
       "      <td>0.832195</td>\n",
       "      <td>-0.530619</td>\n",
       "      <td>1.107323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149133</td>\n",
       "      <td>0.137693</td>\n",
       "      <td>-0.091034</td>\n",
       "      <td>-0.447975</td>\n",
       "      <td>-0.529795</td>\n",
       "      <td>1.272917</td>\n",
       "      <td>-1.244369</td>\n",
       "      <td>-0.682476</td>\n",
       "      <td>0.909858</td>\n",
       "      <td>-1.055520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.436196</td>\n",
       "      <td>-1.428275</td>\n",
       "      <td>-0.242083</td>\n",
       "      <td>-0.275097</td>\n",
       "      <td>-0.119472</td>\n",
       "      <td>0.148710</td>\n",
       "      <td>-0.232974</td>\n",
       "      <td>-0.759262</td>\n",
       "      <td>-0.885669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149133</td>\n",
       "      <td>0.137693</td>\n",
       "      <td>-1.191932</td>\n",
       "      <td>-0.053851</td>\n",
       "      <td>-0.529795</td>\n",
       "      <td>0.878855</td>\n",
       "      <td>-1.244369</td>\n",
       "      <td>-0.682476</td>\n",
       "      <td>1.255292</td>\n",
       "      <td>-0.601030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.618016</td>\n",
       "      <td>-0.218678</td>\n",
       "      <td>-0.256190</td>\n",
       "      <td>-0.251258</td>\n",
       "      <td>-1.063222</td>\n",
       "      <td>-2.462085</td>\n",
       "      <td>-0.446007</td>\n",
       "      <td>-0.759262</td>\n",
       "      <td>1.771653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149133</td>\n",
       "      <td>0.137693</td>\n",
       "      <td>-0.618224</td>\n",
       "      <td>-0.842100</td>\n",
       "      <td>-0.529795</td>\n",
       "      <td>-0.303329</td>\n",
       "      <td>1.211519</td>\n",
       "      <td>-0.682476</td>\n",
       "      <td>-1.403140</td>\n",
       "      <td>-0.044639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RainTomorrow  Location   MinTemp   MaxTemp  Rainfall  Evaporation  \\\n",
       "0             1  0.928162  0.692446 -0.312621 -0.203581     1.264694   \n",
       "1             0 -1.601947  0.205466 -0.707636 -0.275097    -0.371139   \n",
       "2             0 -0.899139 -0.312933 -0.552452 -0.275097    -0.119472   \n",
       "3             0  0.436196 -1.428275 -0.242083 -0.275097    -0.119472   \n",
       "4             0 -0.618016 -0.218678 -0.256190 -0.251258    -1.063222   \n",
       "\n",
       "   Sunshine  WindGustDir  WindGustSpeed  WindDir9am  ...  Cloud9am  Cloud3pm  \\\n",
       "0 -2.063213     1.045228      -0.073333    0.885879  ...  1.464068  1.087018   \n",
       "1  1.236542     1.045228      -0.073333   -1.107113  ...  1.025756 -1.760956   \n",
       "2  0.148710     0.832195      -0.530619    1.107323  ...  0.149133  0.137693   \n",
       "3  0.148710    -0.232974      -0.759262   -0.885669  ...  0.149133  0.137693   \n",
       "4 -2.462085    -0.446007      -0.759262    1.771653  ...  0.149133  0.137693   \n",
       "\n",
       "    Temp9am   Temp3pm  RainToday      year  month_sin  month_cos   day_sin  \\\n",
       "0 -0.029011 -0.593947  -0.529795 -1.879575  -1.244369   0.728636  0.416432   \n",
       "1 -0.230584 -0.623142  -0.529795 -1.485514  -0.725379   1.245139 -0.803968   \n",
       "2 -0.091034 -0.447975  -0.529795  1.272917  -1.244369  -0.682476  0.909858   \n",
       "3 -1.191932 -0.053851  -0.529795  0.878855  -1.244369  -0.682476  1.255292   \n",
       "4 -0.618224 -0.842100  -0.529795 -0.303329   1.211519  -0.682476 -1.403140   \n",
       "\n",
       "    day_cos  \n",
       "0 -1.334442  \n",
       "1  1.199371  \n",
       "2 -1.055520  \n",
       "3 -0.601030  \n",
       "4 -0.044639  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "\n",
    "uri = processing_output_uris[\"test\"] + \"/test.csv\"\n",
    "print(uri.split(\"/\")[2], \"/\".join(uri.split(\"/\")[3:]))\n",
    "\n",
    "obj = s3_client.get_object(Bucket=uri.split(\"/\")[2], Key=\"/\".join(uri.split(\"/\")[3:]))\n",
    "\n",
    "test_data = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "789a32c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " array([-0.89913902, -0.31293265, -0.55245169, -0.27509664, -0.11947224,\n",
       "         0.1487104 ,  0.8321948 , -0.53061881,  1.10732274,  0.48503533,\n",
       "         0.33506294, -0.65044884, -1.16154379,  0.26606081,  0.81076972,\n",
       "         0.80280735,  0.14913321,  0.13769321, -0.0910336 , -0.44797542,\n",
       "        -0.52979545,  1.27291668, -1.24436937, -0.68247589,  0.90985766,\n",
       "        -1.05551973]))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_n = 2\n",
    "\n",
    "row = test_data.iloc[record_n]\n",
    "y = int(row[\"RainTomorrow\"])\n",
    "X = row.drop(\"RainTomorrow\").values\n",
    "y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "299e8f71",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://eu-west-3.console.aws.amazon.com/cloudwatch/home?region=eu-west-3#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-rain-au-model-test in account 543553163241 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15872/2054361806.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://eu-west-3.console.aws.amazon.com/cloudwatch/home?region=eu-west-3#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-rain-au-model-test in account 543553163241 for more information."
     ]
    }
   ],
   "source": [
    "prediction = predictor.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "de51f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(\n",
    "    endpoint_name=\"pytorch-rain-au-model-test\"\n",
    "    # endpoint_name = predictor.endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1558887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603754d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p37",
   "language": "python",
   "name": "conda_pytorch_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
