{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bb42c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping torchvison as it is not installed.\u001b[0m\n",
      "yes: standard output: Broken pipe\n",
      "yes: write error\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.5.1 requires torch<1.10,>=1.7.0, but you have torch 1.10.1 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!yes | pip uninstall torchvison\n",
    "!pip install -qU torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d82485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "947e897e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket : sagemaker-eu-west-3-543553163241/sagemaker/rain-au-pytorch-notebook \n",
      " Role : arn:aws:iam::543553163241:role/service-role/AmazonSageMaker-ExecutionRole-20220116T155750 \n",
      " Session: <sagemaker.session.Session object at 0x7f987ff7c290>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/rain-au-pytorch-notebook'\n",
    "\n",
    "print(f\"Bucket : {bucket}/{prefix} \\n Role : {role} \\n Session: {sagemaker_session}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8075cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eu-west-3'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fbf54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"pipelines/rain/\"\n",
    "input_data_source = \"s3://rain-data-17012022/data/weatherAUS.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c00f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString, ParameterFloat\n",
    "\n",
    "base_job_prefix=\"rain-au\"\n",
    "model_package_group_name = \"RainAuModel\"\n",
    "pipeline_name = \"TrainingPipelineRainAu\"  # SageMaker Pipeline name\n",
    "\n",
    "# parameters for pipeline execution\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\", default_value=1)\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "inference_instance_type = ParameterString(\n",
    "    name=\"InferenceInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "# Training\n",
    "input_data = ParameterString(\n",
    "    name=\"InputDataUrl\",\n",
    "    default_value=input_data_source    # default_value=f\"s3://sagemaker-servicecatalog-seedcode-{region}/dataset/abalone-dataset.csv\",\n",
    ")\n",
    "training_epochs = ParameterString(name=\"TrainingEpochs\", default_value=\"1\")\n",
    "\n",
    "# Validation\n",
    "# Low threshold as it is only an example\n",
    "accuracy_threshold = ParameterFloat(name=\"AccuracyThreshold\", default_value=0.6) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b202ea",
   "metadata": {},
   "source": [
    "# Getting and preprocessing the data\n",
    "Data is stored in an s3 bucket in the account, needs to be processed by an SKlearn processor.\n",
    "\n",
    "First let us update the git repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18209f99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/gmguarino/sagemaker-pipeline-rain-australia-build\r\n",
      " * branch            main       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c907097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a7a50",
   "metadata": {},
   "source": [
    "Examine the preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ee07251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjoblib\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcompose\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ColumnTransformer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mimpute\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SimpleImputer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpipeline\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Pipeline\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StandardScaler, OneHotEncoder, LabelEncoder, StandardScaler\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\r\n",
      "\r\n",
      "logger = logging.getLogger()\r\n",
      "logger.setLevel(logging.INFO)\r\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmerge_two_dicts\u001b[39;49;00m(x, y):\r\n",
      "    \u001b[33m\"\"\"Merges two dicts, returning a new copy.\"\"\"\u001b[39;49;00m\r\n",
      "    z = x.copy()\r\n",
      "    z.update(y)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m z\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcyclical_encode\u001b[39;49;00m(data, col, max_val):\r\n",
      "    data[col + \u001b[33m'\u001b[39;49;00m\u001b[33m_sin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = np.sin(\u001b[34m2\u001b[39;49;00m * np.pi * data[col]/max_val)\r\n",
      "    data[col + \u001b[33m'\u001b[39;49;00m\u001b[33m_cos\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = np.cos(\u001b[34m2\u001b[39;49;00m * np.pi * data[col]/max_val)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m data\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting preprocessing.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# parser = argparse.ArgumentParser()\u001b[39;49;00m\r\n",
      "    \u001b[37m# parser.add_argument(\"--input-data\", type=str, required=True)\u001b[39;49;00m\r\n",
      "    \u001b[37m# args = parser.parse_args()\u001b[39;49;00m\r\n",
      "\r\n",
      "    base_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[37m# pathlib.Path(f\"{base_dir}/data\").mkdir(parents=True, exist_ok=True)\u001b[39;49;00m\r\n",
      "    \u001b[37m# input_data = args.input_data\u001b[39;49;00m\r\n",
      "    \u001b[37m# bucket = input_data.split(\"/\")[2]\u001b[39;49;00m\r\n",
      "    \u001b[37m# key = \"/\".join(input_data.split(\"/\")[3:])\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[37m# logger.debug(\"Downloading data from bucket: %s, key: %s\", bucket, key)\u001b[39;49;00m\r\n",
      "    \u001b[37m# fn = f\"{base_dir}/data/rain-au-dataset.csv\"\u001b[39;49;00m\r\n",
      "    fn = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/input/weatherAUS.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[37m# s3 = boto3.resource(\"s3\")\u001b[39;49;00m\r\n",
      "    \u001b[37m# s3.Bucket(bucket).download_file(key, fn)\u001b[39;49;00m\r\n",
      "    \r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mReading downloaded data.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    df = pd.read_csv(fn)\r\n",
      "    os.unlink(fn)\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mParsing dates as Datetime, and cyclically encoding day and month\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mDate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]= pd.to_datetime(df[\u001b[33m\"\u001b[39;49;00m\u001b[33mDate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33myear\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m\"\u001b[39;49;00m\u001b[33mDate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].dt.year\r\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mmonth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m\"\u001b[39;49;00m\u001b[33mDate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].dt.month\r\n",
      "    df = cyclical_encode(df, \u001b[33m'\u001b[39;49;00m\u001b[33mmonth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m12\u001b[39;49;00m)\r\n",
      "\r\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mday\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m\"\u001b[39;49;00m\u001b[33mDate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].dt.day\r\n",
      "    df = cyclical_encode(df, \u001b[33m'\u001b[39;49;00m\u001b[33mday\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m31\u001b[39;49;00m)\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mHandle missing categorical data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Get list of categorical data\u001b[39;49;00m\r\n",
      "    s = (df.dtypes == \u001b[33m\"\u001b[39;49;00m\u001b[33mobject\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    object_cols = \u001b[36mlist\u001b[39;49;00m(s[s].index)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m object_cols:\r\n",
      "        \u001b[37m# Fill missing data with mode\u001b[39;49;00m\r\n",
      "        df[i].fillna(   df[i].mode()[\u001b[34m0\u001b[39;49;00m], inplace=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mHandle missing numerical data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Get list of numerical data\u001b[39;49;00m\r\n",
      "    t = (df.dtypes == \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat64\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    num_cols = \u001b[36mlist\u001b[39;49;00m(t[t].index)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m num_cols:\r\n",
      "        \u001b[37m# Fill in missing data w/ median\u001b[39;49;00m\r\n",
      "        df[i].fillna(df[i].median(), inplace=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mEncode Categorical Data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    label_encoder = LabelEncoder()\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m object_cols:\r\n",
      "        df[i] = label_encoder.fit_transform(df[i])\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mDropping unnecessary columns, scaling and separating target\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    features = df.drop([\u001b[33m'\u001b[39;49;00m\u001b[33mRainTomorrow\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mDate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mday\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmonth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m) \u001b[37m# dropping target and extra columns\u001b[39;49;00m\r\n",
      "\r\n",
      "    target = df[\u001b[33m'\u001b[39;49;00m\u001b[33mRainTomorrow\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[37m#Set up a standard scaler for the features\u001b[39;49;00m\r\n",
      "    col_names = \u001b[36mlist\u001b[39;49;00m(features.columns)\r\n",
      "    s_scaler = StandardScaler()\r\n",
      "    features = s_scaler.fit_transform(features)\r\n",
      "    features = pd.DataFrame(features, columns=col_names) \r\n",
      "\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mDrop outliers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# reinsert target after scaling\u001b[39;49;00m\r\n",
      "    features[\u001b[33m\"\u001b[39;49;00m\u001b[33mRainTomorrow\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = target\r\n",
      "\r\n",
      "    \u001b[37m#Dropping with outlier\u001b[39;49;00m\r\n",
      "\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mMinTemp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.3\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mMinTemp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2.3\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mMaxTemp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.3\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mMaxTemp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mRainfall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m4.5\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mEvaporation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.8\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mSunshine\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.1\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mWindGustSpeed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m4\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mWindGustSpeed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m4\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mWindSpeed9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m4\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mWindSpeed3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.5\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mHumidity9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m3\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mHumidity3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2.2\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mPressure9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]< \u001b[34m2\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mPressure9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2.7\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mPressure3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]< \u001b[34m2\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mPressure3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2.7\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mCloud9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m1.8\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mCloud3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mTemp9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.3\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mTemp9am\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2\u001b[39;49;00m)]\r\n",
      "    features = features[(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mTemp3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]<\u001b[34m2.3\u001b[39;49;00m)&(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mTemp3pm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]>-\u001b[34m2\u001b[39;49;00m)]\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mSplitting data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[37m# Notebook code\u001b[39;49;00m\r\n",
      "    X = features.drop([\u001b[33m\"\u001b[39;49;00m\u001b[33mRainTomorrow\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "    y = features[\u001b[33m\"\u001b[39;49;00m\u001b[33mRainTomorrow\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[37m# Splitting test and training sets\u001b[39;49;00m\r\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = \u001b[34m0.3\u001b[39;49;00m)\r\n",
      "    X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size = \u001b[34m0.5\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# concatenate fo caching\u001b[39;49;00m\r\n",
      "    train = pd.concat([y_train, X_train], axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "    validation = pd.concat([y_dev, X_dev], axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "    test = pd.concat([y_test, X_test], axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# SM template code\u001b[39;49;00m\r\n",
      "    \u001b[37m# logger.info(\"Applying transforms.\")\u001b[39;49;00m\r\n",
      "    \u001b[37m# y = df.pop(\"rings\")\u001b[39;49;00m\r\n",
      "    \u001b[37m# X_pre = preprocess.fit_transform(df)\u001b[39;49;00m\r\n",
      "    \u001b[37m# y_pre = y.to_numpy().reshape(len(y), 1)\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# X = np.concatenate((y_pre, X_pre), axis=1)\u001b[39;49;00m\r\n",
      "    \u001b[37m# X = features.to_numpy()\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# logger.info(\"Splitting %d rows of data into train, validation, test datasets.\", len(X))\u001b[39;49;00m\r\n",
      "    \u001b[37m# np.random.shuffle(features)\u001b[39;49;00m\r\n",
      "    \u001b[37m# train, validation, test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])\u001b[39;49;00m\r\n",
      "\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mWriting out datasets to \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, base_dir)\r\n",
      "    train.to_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/train/train.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, header=\u001b[34mTrue\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "    validation.to_csv(\r\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/validation/validation.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, header=\u001b[34mTrue\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    test.to_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/test/test.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, header=\u001b[34mTrue\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "    pathlib.Path(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/preprocess\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).mkdir(parents=\u001b[34mTrue\u001b[39;49;00m, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    joblib.dump(s_scaler, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tarfile.open(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbase_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/preprocess/model.tar.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mw:gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m tf:\r\n",
      "        tf.add(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(input_data, content_type):\r\n",
      "    \u001b[33m\"\"\"Parse input data payload\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    We currently only take csv input. Since we need to process both labelled\u001b[39;49;00m\r\n",
      "\u001b[33m    and unlabelled data we first determine whether the label column is present\u001b[39;49;00m\r\n",
      "\u001b[33m    by looking at how many columns were provided.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m\"\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "        \u001b[37m# Read the raw input data as CSV.\u001b[39;49;00m\r\n",
      "        df = pd.read_csv(StringIO(input_data), header=\u001b[34mNone\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(df.columns) == \u001b[36mlen\u001b[39;49;00m(feature_columns) + \u001b[34m1\u001b[39;49;00m:\r\n",
      "            \u001b[37m# This is a labelled example, includes the ring label\u001b[39;49;00m\r\n",
      "            df.columns = feature_columns + [label_column]\r\n",
      "        \u001b[34melif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(df.columns) == \u001b[36mlen\u001b[39;49;00m(feature_columns):\r\n",
      "            \u001b[37m# This is an unlabelled example.\u001b[39;49;00m\r\n",
      "            df.columns = feature_columns\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m df\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m not supported by script!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(content_type))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction, accept):\r\n",
      "    \u001b[33m\"\"\"Format prediction output\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    The default accept/content-type between containers for serial inference is JSON.\u001b[39;49;00m\r\n",
      "\u001b[33m    We also want to set the ContentType or mimetype as the same value as accept so the next\u001b[39;49;00m\r\n",
      "\u001b[33m    container can read the response payload correctly.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m accept == \u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "        instances = []\r\n",
      "        \u001b[34mfor\u001b[39;49;00m row \u001b[35min\u001b[39;49;00m prediction.tolist():\r\n",
      "            instances.append(row)\r\n",
      "        json_output = {\u001b[33m\"\u001b[39;49;00m\u001b[33minstances\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: instances}\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m worker.Response(json.dumps(json_output), mimetype=accept)\r\n",
      "    \u001b[34melif\u001b[39;49;00m accept == \u001b[33m\"\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m worker.Response(encoders.encode(prediction, accept), mimetype=accept)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m RuntimeException(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m accept type is not supported by this script.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(accept))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\r\n",
      "    \u001b[33m\"\"\"Preprocess input data\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    We implement this because the default predict_fn uses .predict(), but our model is a preprocessor\u001b[39;49;00m\r\n",
      "\u001b[33m    so we want to use .transform().\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    The output is returned in the following order:\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        rest of features either one hot encoded or standardized\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    features = model.transform(input_data)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m label_column \u001b[35min\u001b[39;49;00m input_data:\r\n",
      "        \u001b[37m# Return the label (as the first column) and the set of features.\u001b[39;49;00m\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m np.insert(features, \u001b[34m0\u001b[39;49;00m, input_data[label_column], axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[37m# Return only the set of features\u001b[39;49;00m\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m features\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    \u001b[33m\"\"\"Deserialize fitted model\"\"\"\u001b[39;49;00m\r\n",
      "    preprocessor = joblib.load(os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m preprocessor\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize pipelines/rain/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f512d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=sklearn_framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    volume_size_in_gb=15,\n",
    "    base_job_name=\"rainau-notebook-pipeline-preprocess\",\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "931f59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_step = ProcessingStep(\n",
    "    name=\"RainAuProcessing\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[ProcessingInput(source=input_data,\n",
    "        destination='/opt/ml/processing/input/')],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\",\n",
    "            source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\",\n",
    "            source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\",\n",
    "            source=\"/opt/ml/processing/test\"),\n",
    "        ProcessingOutput(output_name=\"scaler_model\",\n",
    "            source=\"/opt/ml/processing/preprocess\")\n",
    "    ],\n",
    "    code=os.path.join(BASE_DIR, \"preprocess.py\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb51c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"s3://{sagemaker_session.default_bucket()}/{base_job_prefix}/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebada23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-west-3-543553163241/rain-au/model'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5d0209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f05c125",
   "metadata": {},
   "source": [
    "# Train the model in a Pytorch container\n",
    "\n",
    "The script `train.py` takes care of the training and saving of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa177f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/gmguarino/sagemaker-pipeline-rain-australia-build\r\n",
      " * branch            main       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2784aa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m#import sagemaker_containers\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dataset\r\n",
      "\r\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\r\n",
      "logger.setLevel(logging.INFO)\r\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m# Dataset class to handle data loading\u001b[39;49;00m\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mRainDataset\u001b[39;49;00m(Dataset):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, csv_file, root_dir):\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data = pd.read_csv(os.path.join(root_dir, csv_file))\r\n",
      "        \u001b[36mself\u001b[39;49;00m.labels = np.asarray(\u001b[36mself\u001b[39;49;00m.data.iloc[:, \u001b[34m0\u001b[39;49;00m])\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.labels)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, idx):\r\n",
      "        \u001b[34mif\u001b[39;49;00m torch.is_tensor(idx):\r\n",
      "            idx = idx.tolist()\r\n",
      "\r\n",
      "        single_label = \u001b[36mself\u001b[39;49;00m.labels[idx]\r\n",
      "\r\n",
      "        data_array = np.asarray(\u001b[36mself\u001b[39;49;00m.data.iloc[idx, \u001b[34m1\u001b[39;49;00m:])\r\n",
      "        feature_tensor = torch.tensor(data_array, dtype=torch.float32)\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m (feature_tensor, np.float32(single_label))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m#  Simple ANN for binary classification\u001b[39;49;00m\r\n",
      "\u001b[37m# Who cares this is a demo for sagemaker not a DL demo\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc1 = nn.Linear(\u001b[34m26\u001b[39;49;00m, \u001b[34m32\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc2 = nn.Linear(\u001b[34m32\u001b[39;49;00m, \u001b[34m32\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc3 = nn.Linear(\u001b[34m32\u001b[39;49;00m, \u001b[34m16\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc4 = nn.Linear(\u001b[34m16\u001b[39;49;00m, \u001b[34m8\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.out = nn.Linear(\u001b[34m8\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc1(x))\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc2(x))\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc3(x))\r\n",
      "        x = F.dropout(x, p=\u001b[34m0.25\u001b[39;49;00m, training=\u001b[36mself\u001b[39;49;00m.training)\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc4(x))\r\n",
      "        x = F.dropout(x, p=\u001b[34m0.5\u001b[39;49;00m, training=\u001b[36mself\u001b[39;49;00m.training)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m F.sigmoid(\u001b[36mself\u001b[39;49;00m.out(x))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(batch_size, training_dir, is_distributed, **kwargs):\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    dataset = RainDataset(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        training_dir\r\n",
      "    )\r\n",
      "    train_sampler = (\r\n",
      "        torch.utils.data.distributed.DistributedSampler(dataset) \u001b[34mif\u001b[39;49;00m is_distributed \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(\r\n",
      "        dataset,\r\n",
      "        batch_size=batch_size,\r\n",
      "        shuffle=train_sampler \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\r\n",
      "        sampler=train_sampler,\r\n",
      "        **kwargs\r\n",
      "    )\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_average_gradients\u001b[39;49;00m(model):\r\n",
      "    \u001b[37m# Gradient averaging.\u001b[39;49;00m\r\n",
      "    size = \u001b[36mfloat\u001b[39;49;00m(dist.get_world_size())\r\n",
      "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\r\n",
      "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\r\n",
      "        param.grad.data /= size\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmake_tarfile\u001b[39;49;00m(output_filename, source_dir):\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tarfile.open(output_filename, \u001b[33m\"\u001b[39;49;00m\u001b[33mw:gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m tar:\r\n",
      "        \u001b[34mfor\u001b[39;49;00m filename \u001b[35min\u001b[39;49;00m os.listdir(source_dir):\r\n",
      "            tar.add(os.path.join(source_dir, filename), arcname=filename)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\r\n",
      "    is_distributed = \u001b[36mlen\u001b[39;49;00m(args.hosts) > \u001b[34m1\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m args.backend \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mDistributed training - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(is_distributed))\r\n",
      "    use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of gpus available - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.num_gpus))\r\n",
      "    kwargs = {\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m} \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m {}\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed:\r\n",
      "        \u001b[37m# Initialize the distributed environment.\u001b[39;49;00m\r\n",
      "        world_size = \u001b[36mlen\u001b[39;49;00m(args.hosts)\r\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mWORLD_SIZE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(world_size)\r\n",
      "        host_rank = args.hosts.index(args.current_host)\r\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mRANK\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(host_rank)\r\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\r\n",
      "        logger.info(\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mInitialized the distributed environment: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m backend on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m nodes. \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "                args.backend, dist.get_world_size()\r\n",
      "            )\r\n",
      "            + \u001b[33m\"\u001b[39;49;00m\u001b[33mCurrent host rank is \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m. Number of gpus: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(dist.get_rank(), args.num_gpus)\r\n",
      "        )\r\n",
      "\r\n",
      "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\r\n",
      "    torch.manual_seed(args.seed)\r\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\r\n",
      "        torch.cuda.manual_seed(args.seed)\r\n",
      "\r\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed, **kwargs)\r\n",
      "\r\n",
      "    logger.debug(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "            \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\r\n",
      "            \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\r\n",
      "            \u001b[34m100.0\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(train_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\r\n",
      "        )\r\n",
      "    )\r\n",
      "\r\n",
      "\r\n",
      "    model = Net().to(device)\r\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m use_cuda:\r\n",
      "        \u001b[37m# multi-machine multi-gpu case\u001b[39;49;00m\r\n",
      "        model = torch.nn.parallel.DistributedDataParallel(model)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[37m# single-machine multi-gpu case or single-machine or multi-machine cpu case\u001b[39;49;00m\r\n",
      "        model = torch.nn.DataParallel(model)\r\n",
      "\r\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\r\n",
      "\r\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\r\n",
      "        model.train()\r\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\r\n",
      "            data, target = data.to(device), target.to(device)\r\n",
      "            optimizer.zero_grad()\r\n",
      "            output = model(data)\r\n",
      "            loss = F.binary_cross_entropy(output.squeeze(), target.squeeze())\r\n",
      "            loss.backward()\r\n",
      "            \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m use_cuda:\r\n",
      "                \u001b[37m# average gradients manually for multi-machine cpu case only\u001b[39;49;00m\r\n",
      "                _average_gradients(model)\r\n",
      "            optimizer.step()\r\n",
      "            \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m:\r\n",
      "                logger.info(\r\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "                        epoch,\r\n",
      "                        batch_idx * \u001b[36mlen\u001b[39;49;00m(data),\r\n",
      "                        \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\r\n",
      "                        \u001b[34m100.0\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader),\r\n",
      "                        loss.item(),\r\n",
      "                    )\r\n",
      "                )\r\n",
      "    save_model(model, args.model_dir)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model = torch.nn.DataParallel(Net())\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\r\n",
      "    \u001b[37m# Model is saved both as a deployment torchscript as well as a normal pth file\u001b[39;49;00m\r\n",
      "    \u001b[37m# for evaluation in the pipeline\u001b[39;49;00m\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    path = os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    torch.save(model.cpu().state_dict(), path)\r\n",
      "    \u001b[37m# inference_code_path = model_dir + \"/code/\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# # if not os.path.exists(inference_code_path):\u001b[39;49;00m\r\n",
      "    \u001b[37m# #     os.mkdir(inference_code_path)\u001b[39;49;00m\r\n",
      "    \u001b[37m# #     logger.info(\"Created a folder at {}!\".format(inference_code_path))\u001b[39;49;00m\r\n",
      "    \u001b[37m# pathlib.Path(inference_code_path).mkdir(parents=True, exist_ok=True)\u001b[39;49;00m\r\n",
      "    \u001b[37m# base_dir = os.path.dirname(os.path.realpath(__file__))\u001b[39;49;00m\r\n",
      "    \u001b[37m# shutil.copy(os.path.join(base_dir,\"inference.py\"), inference_code_path)\u001b[39;49;00m\r\n",
      "    \u001b[37m# make_tarfile(os.path.join(model_dir, \"model.tar.gz\"), model_dir)\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m10\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.00009\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 0.00009)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m100\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "\r\n",
      "    train(parser.parse_args())\r\n",
      "\r\n",
      "    \u001b[37m# TODO: REMOVE JIT LEAVE AS NORMAL MODEL\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize pipelines/rain/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a41ea8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    source_dir=BASE_DIR,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version='1.8.0',\n",
    "    py_version='py3',\n",
    "    base_job_name=f\"{base_job_prefix}-torch-train-notebook-pipeline\",\n",
    "    output_path=model_path,\n",
    "    hyperparameters={'epochs': training_epochs, 'batch-size': 32, 'lr': 0.00009},\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1926f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"RainAuPytorchModel\",\n",
    "    estimator=pytorch_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805caaba",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8b00db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/gmguarino/sagemaker-pipeline-rain-australia-build\r\n",
      " * branch            main       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "734d9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, FrameworkProcessor\n",
    "from sagemaker.workflow.properties import PropertyFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ac013de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"Evaluation script for measuring mean squared error.\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m#import sagemaker_containers\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dataset\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (accuracy_score, \r\n",
      "    precision_score, recall_score, f1_score)\r\n",
      "\r\n",
      "logger = logging.getLogger()\r\n",
      "logger.setLevel(logging.INFO)\r\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\r\n",
      "\r\n",
      "\u001b[37m# Dataset class to handle data loading\u001b[39;49;00m\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mRainDataset\u001b[39;49;00m(Dataset):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, csv_file, root_dir):\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data = pd.read_csv(os.path.join(root_dir, csv_file))\r\n",
      "        \u001b[36mself\u001b[39;49;00m.labels = np.asarray(\u001b[36mself\u001b[39;49;00m.data.iloc[:, \u001b[34m0\u001b[39;49;00m])\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.labels)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, idx):\r\n",
      "        \u001b[34mif\u001b[39;49;00m torch.is_tensor(idx):\r\n",
      "            idx = idx.tolist()\r\n",
      "\r\n",
      "        single_label = \u001b[36mself\u001b[39;49;00m.labels[idx]\r\n",
      "\r\n",
      "        data_array = np.asarray(\u001b[36mself\u001b[39;49;00m.data.iloc[idx, \u001b[34m1\u001b[39;49;00m:])\r\n",
      "        feature_tensor = torch.tensor(data_array, dtype=torch.float32)\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m (feature_tensor, np.float32(single_label))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m#  Simple ANN for binary classification\u001b[39;49;00m\r\n",
      "\u001b[37m# Who cares this is a demo for sagemaker not a DL demo\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc1 = nn.Linear(\u001b[34m26\u001b[39;49;00m, \u001b[34m32\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc2 = nn.Linear(\u001b[34m32\u001b[39;49;00m, \u001b[34m32\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc3 = nn.Linear(\u001b[34m32\u001b[39;49;00m, \u001b[34m16\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc4 = nn.Linear(\u001b[34m16\u001b[39;49;00m, \u001b[34m8\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.out = nn.Linear(\u001b[34m8\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc1(x))\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc2(x))\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc3(x))\r\n",
      "        x = F.dropout(x, p=\u001b[34m0.25\u001b[39;49;00m, training=\u001b[36mself\u001b[39;49;00m.training)\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc4(x))\r\n",
      "        x = F.dropout(x, p=\u001b[34m0.5\u001b[39;49;00m, training=\u001b[36mself\u001b[39;49;00m.training)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m torch.sigmoid(\u001b[36mself\u001b[39;49;00m.out(x))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_test_data_loader\u001b[39;49;00m(test_batch_size, test_dir, test_file=\u001b[33m\"\u001b[39;49;00m\u001b[33mtest.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, **kwargs):\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet test data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    dataset = RainDataset(\r\n",
      "        test_file,\r\n",
      "        test_dir\r\n",
      "    )\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(\r\n",
      "        dataset,\r\n",
      "        batch_size=test_batch_size,\r\n",
      "        shuffle=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "        **kwargs\r\n",
      "    )\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mformat_target\u001b[39;49;00m(target):\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m target.to(\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).squeeze().numpy().round().astype(np.uint)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, test_loader, device):\r\n",
      "    model.eval()\r\n",
      "    test_loss = \u001b[34m0\u001b[39;49;00m\r\n",
      "    correct = \u001b[34m0\u001b[39;49;00m\r\n",
      "    result_dict = {\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:[], \u001b[33m\"\u001b[39;49;00m\u001b[33mprecision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:[], \u001b[33m\"\u001b[39;49;00m\u001b[33mrecall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:[], \u001b[33m\"\u001b[39;49;00m\u001b[33mf1_score\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:[]}\r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target \u001b[35min\u001b[39;49;00m test_loader:\r\n",
      "            data, target = data.to(device), target.to(device)\r\n",
      "            output = model(data)\r\n",
      "            test_loss += F.binary_cross_entropy(output.squeeze(), target.squeeze(), reduction=\u001b[33m\"\u001b[39;49;00m\u001b[33msum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).item()  \u001b[37m# sum up batch loss\u001b[39;49;00m\r\n",
      "            result_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].append(accuracy_score(format_target(target), format_target(output)))\r\n",
      "            result_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mprecision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].append(precision_score(format_target(target), format_target(output)))\r\n",
      "            result_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mrecall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].append(recall_score(format_target(target), format_target(output)))\r\n",
      "            result_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mf1_score\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].append(f1_score(format_target(target), format_target(output)))\r\n",
      "\r\n",
      "    \u001b[34mfor\u001b[39;49;00m key \u001b[35min\u001b[39;49;00m result_dict.keys():\r\n",
      "        mean = \u001b[36msum\u001b[39;49;00m(result_dict[key]) / \u001b[36mlen\u001b[39;49;00m(result_dict[key])\r\n",
      "        result_dict[key] = mean\r\n",
      "            \r\n",
      "    test_loss /= \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\r\n",
      "    logger.info(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mTest set stats: \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m Num examples: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m,  \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(result_dict, result_dict)\r\n",
      "    )\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33mregression_metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: result_dict}\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    model = Net()\r\n",
      "    model = torch.nn.DataParallel(model)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tarfile.open(os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mr:gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m tar:\r\n",
      "        tar.extractall(\u001b[33m\"\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m1000\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    args = parser.parse_args()\r\n",
      "    num_gpus = torch.cuda.device_count()\r\n",
      "\r\n",
      "    use_cuda = num_gpus > \u001b[34m0\u001b[39;49;00m\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of gpus available - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(num_gpus))\r\n",
      "    kwargs = {\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m} \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m {}\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading Model...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/model/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    model = model_fn(model_dir=model_path).to(device)\r\n",
      "    \u001b[37m# if num_gpus > 1:\u001b[39;49;00m\r\n",
      "    \u001b[37m# model = torch.nn.DataParallel(model)\u001b[39;49;00m\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading DataLoader...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    validation_folder = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    test_loader = _get_test_data_loader(args.test_batch_size, validation_folder, \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mTesting!!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    result_dict = test(model, test_loader, device)\r\n",
      "\r\n",
      "    output_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/evaluation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    pathlib.Path(output_dir).mkdir(parents=\u001b[34mTrue\u001b[39;49;00m, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    evaluation_path = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00moutput_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/evaluation.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(evaluation_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        f.write(json.dumps(result_dict))\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize pipelines/rain/evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62954bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pytorch_processor = FrameworkProcessor(\n",
    "#    PyTorch,\n",
    "#    instance_type=processing_instance_type,\n",
    "#    instance_count=1,\n",
    "#    framework_version='1.8.0',\n",
    "#    base_job_name=f\"{base_job_prefix}-torch-eval\",\n",
    "#    sagemaker_session=sagemaker_session,\n",
    "#    role=role,\n",
    "#    command=[\"python3\"],\n",
    "#)\n",
    "\n",
    "pytorch_eval_image = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version='1.8.0',\n",
    "    image_scope=\"training\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=training_instance_type\n",
    ")\n",
    "\n",
    "evaluation_processor = ScriptProcessor(\n",
    "    role=role,\n",
    "    image_uri=pytorch_eval_image,\n",
    "    command=[\"python3\"],\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type\n",
    ")\n",
    "\n",
    "#References outputs from the processing step\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "step_evaluate = ProcessingStep(\n",
    "    name=\"EvaluatePerformance\",\n",
    "    processor=evaluation_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=processing_step.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/validation\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    property_files=[evaluation_report],\n",
    "    code=os.path.join(BASE_DIR, \"evaluate.py\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7d639",
   "metadata": {},
   "source": [
    "# Register the model on condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ce7fc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-west-3-543553163241/RainAuProcessing-e1284f24b646de14ac1aa45ecdee541d/output/scaler_model/model.tar.gz'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_model_artifacts = \"{}/model.tar.gz\".format(\n",
    "    processing_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][3][\"S3Output\"][\"S3Uri\"]\n",
    ")\n",
    "scaler_model_artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f39497e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Load the models for registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b97c1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker import PipelineModel\n",
    "\n",
    "# Load the scaler sklearn model artifacts into a SKLearnModel class\n",
    "\n",
    "scaler_model = SKLearnModel(\n",
    "    model_data=scaler_model_artifacts,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    entry_point=os.path.join(BASE_DIR, \"preprocess.py\"), # The handler functions are defined here as well\n",
    "    framework_version=sklearn_framework_version, # as before\n",
    ")\n",
    "\n",
    "\n",
    "# Load the pytorch model artifacts\n",
    "pytorch_model = PyTorchModel(\n",
    "    entry_point='inference.py',\n",
    "    source_dir=BASE_DIR,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=role,\n",
    "    framework_version='1.8.0',\n",
    "    py_version='py3',\n",
    "    name=\"rain-au-inference-model-pipeline\",\n",
    "    sagemaker_session=sagemaker_session # remember to always have this\n",
    ")\n",
    "\n",
    "# Combine them into a single model\n",
    "pipeline_model = PipelineModel(\n",
    "    models=[scaler_model, pytorch_model], role=role, sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3bbda0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.session.Session at 0x7f987ff7c290>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "058a1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registration step\n",
    "\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "\n",
    "evaluation_s3_uri = \"{}/evaluation.json\".format(\n",
    "    step_evaluate.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    ")\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=evaluation_s3_uri,\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "step_register_pipeline_model = RegisterModel(\n",
    "    name=\"RainAuPipelineModel\",\n",
    "    model=pipeline_model,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.m5.large\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    model_metrics=model_metrics,\n",
    "    approval_status=model_approval_status,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4801c6d5",
   "metadata": {},
   "source": [
    "# Create condition for registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "73f5419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/gmguarino/sagemaker-pipeline-rain-australia-build\r\n",
      " * branch            main       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "761e31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "29b25692",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# basically this check that <left> <= <right>\n",
    "condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet( # Basically this reads the property file to find the necessary metric\n",
    "        step_name=step_evaluate.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.accuracy\", # probably should change path as it is not regression\n",
    "    ),\n",
    "    right=accuracy_threshold\n",
    ")\n",
    "\n",
    "# Turn this into a pipeline step\n",
    "step_cond = ConditionStep(\n",
    "    name=\"rain-au-accuracy-condition\",\n",
    "    conditions=[condition],\n",
    "    if_steps=[step_register_pipeline_model],  # step_register_model, step_register_scaler,\n",
    "    else_steps=[], # if the model does not pass then nothing happens\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3cb100",
   "metadata": {},
   "source": [
    "# Actually create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6356025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Create a Sagemaker Pipeline.\n",
    "# Each parameter for the pipeline must be set as a parameter explicitly when the pipeline is created.\n",
    "# Also pass in each of the steps created above.\n",
    "# Note that the order of execution is determined from each step's dependencies on other steps,\n",
    "# not on the order they are passed in below.\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[ # These are all the parameters defined at the beginning that can be passed and adapted\n",
    "        # as the pipelines are executed\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        inference_instance_type,\n",
    "        input_data,\n",
    "        model_approval_status,\n",
    "        training_epochs,\n",
    "        accuracy_threshold,\n",
    "    ],\n",
    "    steps=[processing_step, step_train, step_evaluate, step_cond]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12f726",
   "metadata": {},
   "source": [
    "# View and inspect pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "61281c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'InferenceInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'InputDataUrl',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://rain-data-17012022/data/weatherAUS.csv'},\n",
       "  {'Name': 'ModelApprovalStatus',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'PendingManualApproval'},\n",
       "  {'Name': 'TrainingEpochs', 'Type': 'String', 'DefaultValue': '1'},\n",
       "  {'Name': 'AccuracyThreshold', 'Type': 'Float', 'DefaultValue': 0.6}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'RainAuProcessing',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 15}},\n",
       "    'AppSpecification': {'ImageUri': '659782779980.dkr.ecr.eu-west-3.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocess.py']},\n",
       "    'RoleArn': 'arn:aws:iam::543553163241:role/service-role/AmazonSageMaker-ExecutionRole-20220116T155750',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Parameters.InputDataUrl'},\n",
       "       'LocalPath': '/opt/ml/processing/input/',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/RainAuProcessing-e1284f24b646de14ac1aa45ecdee541d/input/code/preprocess.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/RainAuProcessing-e1284f24b646de14ac1aa45ecdee541d/output/train',\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'validation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/RainAuProcessing-e1284f24b646de14ac1aa45ecdee541d/output/validation',\n",
       "        'LocalPath': '/opt/ml/processing/validation',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/RainAuProcessing-e1284f24b646de14ac1aa45ecdee541d/output/test',\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'scaler_model',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/RainAuProcessing-e1284f24b646de14ac1aa45ecdee541d/output/scaler_model',\n",
       "        'LocalPath': '/opt/ml/processing/preprocess',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'RainAuPytorchModel',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '763104351884.dkr.ecr.eu-west-3.amazonaws.com/pytorch-training:1.8.0-cpu-py3',\n",
       "     'EnableSageMakerMetricsTimeSeries': True},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-eu-west-3-543553163241/rain-au/model'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'InstanceCount': 1,\n",
       "     'InstanceType': {'Get': 'Parameters.TrainingInstanceType'},\n",
       "     'VolumeSizeInGB': 30},\n",
       "    'RoleArn': 'arn:aws:iam::543553163241:role/service-role/AmazonSageMaker-ExecutionRole-20220116T155750',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.RainAuProcessing.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'train'}],\n",
       "    'HyperParameters': {'epochs': {'Get': 'Parameters.TrainingEpochs'},\n",
       "     'batch-size': '32',\n",
       "     'lr': '9e-05',\n",
       "     'sagemaker_submit_directory': '\"s3://sagemaker-eu-west-3-543553163241/rain-au-torch-train-notebook-pipeline-2022-01-24-16-34-15-292/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"train.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_job_name': '\"rain-au-torch-train-notebook-pipeline-2022-01-24-16-34-15-292\"',\n",
       "     'sagemaker_region': '\"eu-west-3\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-eu-west-3-543553163241/rain-au/model',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerRuleConfigurations': [{'RuleConfigurationName': 'ProfilerReport-1643042055',\n",
       "      'RuleEvaluatorImage': '447278800020.dkr.ecr.eu-west-3.amazonaws.com/sagemaker-debugger-rules:latest',\n",
       "      'RuleParameters': {'rule_to_invoke': 'ProfilerReport'}}],\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-eu-west-3-543553163241/rain-au/model'}}},\n",
       "  {'Name': 'EvaluatePerformance',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.TrainingInstanceType'},\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '763104351884.dkr.ecr.eu-west-3.amazonaws.com/pytorch-training:1.8.0-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/evaluate.py']},\n",
       "    'RoleArn': 'arn:aws:iam::543553163241:role/service-role/AmazonSageMaker-ExecutionRole-20220116T155750',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Steps.RainAuPytorchModel.ModelArtifacts.S3ModelArtifacts'},\n",
       "       'LocalPath': '/opt/ml/processing/model',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'input-2',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.RainAuProcessing.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/validation',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/EvaluatePerformance-c80c4963fbba66f1a47de8c9c9893092/input/code/evaluate.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'evaluation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-eu-west-3-543553163241/EvaluatePerformance-c80c4963fbba66f1a47de8c9c9893092/output/evaluation',\n",
       "        'LocalPath': '/opt/ml/processing/evaluation',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'PropertyFiles': [{'PropertyFileName': 'EvaluationReport',\n",
       "     'OutputName': 'evaluation',\n",
       "     'FilePath': 'evaluation.json'}]},\n",
       "  {'Name': 'rain-au-accuracy-condition',\n",
       "   'Type': 'Condition',\n",
       "   'Arguments': {'Conditions': [{'Type': 'GreaterThanOrEqualTo',\n",
       "      'LeftValue': {'Std:JsonGet': {'PropertyFile': {'Get': 'Steps.EvaluatePerformance.PropertyFiles.EvaluationReport'},\n",
       "        'Path': 'regression_metrics.accuracy'}},\n",
       "      'RightValue': {'Get': 'Parameters.AccuracyThreshold'}}],\n",
       "    'IfSteps': [{'Name': 'sklearnRepackModel',\n",
       "      'Type': 'Training',\n",
       "      'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "        'TrainingImage': '659782779980.dkr.ecr.eu-west-3.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
       "       'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-eu-west-3-543553163241/'},\n",
       "       'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "       'ResourceConfig': {'InstanceCount': 1,\n",
       "        'InstanceType': 'ml.m5.large',\n",
       "        'VolumeSizeInGB': 30},\n",
       "       'RoleArn': 'arn:aws:iam::543553163241:role/service-role/AmazonSageMaker-ExecutionRole-20220116T155750',\n",
       "       'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "           'S3Uri': 's3://sagemaker-eu-west-3-543553163241/RainAuProcessing-e1284f24b646de14ac1aa45ecdee541d/output/scaler_model',\n",
       "           'S3DataDistributionType': 'FullyReplicated'}},\n",
       "         'ChannelName': 'training'}],\n",
       "       'HyperParameters': {'inference_script': '\"preprocess.py\"',\n",
       "        'model_archive': '\"model.tar.gz\"',\n",
       "        'dependencies': 'null',\n",
       "        'source_dir': 'null',\n",
       "        'sagemaker_submit_directory': '\"s3://sagemaker-eu-west-3-543553163241/sagemaker-scikit-learn-2022-01-24-16-34-15-469/source/sourcedir.tar.gz\"',\n",
       "        'sagemaker_program': '\"_repack_model.py\"',\n",
       "        'sagemaker_container_log_level': '20',\n",
       "        'sagemaker_job_name': '\"sagemaker-scikit-learn-2022-01-24-16-34-15-469\"',\n",
       "        'sagemaker_region': '\"eu-west-3\"'},\n",
       "       'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-eu-west-3-543553163241/',\n",
       "        'CollectionConfigurations': []}}},\n",
       "     {'Name': 'rain-au-inference-model-pipelineRepackModel',\n",
       "      'Type': 'Training',\n",
       "      'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "        'TrainingImage': '659782779980.dkr.ecr.eu-west-3.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
       "       'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-eu-west-3-543553163241/'},\n",
       "       'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "       'ResourceConfig': {'InstanceCount': 1,\n",
       "        'InstanceType': 'ml.m5.large',\n",
       "        'VolumeSizeInGB': 30},\n",
       "       'RoleArn': 'arn:aws:iam::543553163241:role/service-role/AmazonSageMaker-ExecutionRole-20220116T155750',\n",
       "       'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "           'S3Uri': {'Get': 'Steps.RainAuPytorchModel.ModelArtifacts.S3ModelArtifacts'},\n",
       "           'S3DataDistributionType': 'FullyReplicated'}},\n",
       "         'ChannelName': 'training'}],\n",
       "       'HyperParameters': {'inference_script': '\"inference.py\"',\n",
       "        'model_archive': '\"model.tar.gz\"',\n",
       "        'dependencies': 'null',\n",
       "        'source_dir': '\"pipelines/rain/\"',\n",
       "        'sagemaker_submit_directory': '\"s3://sagemaker-eu-west-3-543553163241/sagemaker-scikit-learn-2022-01-24-16-34-15-542/source/sourcedir.tar.gz\"',\n",
       "        'sagemaker_program': '\"_repack_model.py\"',\n",
       "        'sagemaker_container_log_level': '20',\n",
       "        'sagemaker_job_name': '\"sagemaker-scikit-learn-2022-01-24-16-34-15-542\"',\n",
       "        'sagemaker_region': '\"eu-west-3\"'},\n",
       "       'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-eu-west-3-543553163241/',\n",
       "        'CollectionConfigurations': []}}},\n",
       "     {'Name': 'RainAuPipelineModel',\n",
       "      'Type': 'RegisterModel',\n",
       "      'Arguments': {'ModelPackageGroupName': 'RainAuModel',\n",
       "       'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
       "          'S3Uri': 's3://sagemaker-eu-west-3-543553163241/EvaluatePerformance-c80c4963fbba66f1a47de8c9c9893092/output/evaluation/evaluation.json'}},\n",
       "        'Bias': {},\n",
       "        'Explainability': {}},\n",
       "       'InferenceSpecification': {'Containers': [{'Image': '659782779980.dkr.ecr.eu-west-3.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "          'Environment': {'SAGEMAKER_PROGRAM': 'preprocess.py',\n",
       "           'SAGEMAKER_SUBMIT_DIRECTORY': 's3://sagemaker-eu-west-3-543553163241/sagemaker-scikit-learn-2022-01-24-15-41-47-116/sourcedir.tar.gz',\n",
       "           'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "           'SAGEMAKER_REGION': 'eu-west-3'},\n",
       "          'ModelDataUrl': {'Get': 'Steps.sklearnRepackModel.ModelArtifacts.S3ModelArtifacts'}},\n",
       "         {'Image': '763104351884.dkr.ecr.eu-west-3.amazonaws.com/pytorch-inference:1.8.0-cpu-py3',\n",
       "          'Environment': {'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "           'SAGEMAKER_SUBMIT_DIRECTORY': 'file://pipelines/rain/',\n",
       "           'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "           'SAGEMAKER_REGION': 'eu-west-3'},\n",
       "          'ModelDataUrl': {'Get': 'Steps.rain-au-inference-model-pipelineRepackModel.ModelArtifacts.S3ModelArtifacts'}}],\n",
       "        'SupportedContentTypes': ['application/json'],\n",
       "        'SupportedResponseMIMETypes': ['application/json'],\n",
       "        'SupportedRealtimeInferenceInstanceTypes': ['ml.m5.large',\n",
       "         'ml.m5.xlarge'],\n",
       "        'SupportedTransformInstanceTypes': ['ml.m5.xlarge']},\n",
       "       'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'}}}],\n",
       "    'ElseSteps': []}}]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913a6687",
   "metadata": {},
   "source": [
    "# Submit pipeline to Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aa6ce949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:eu-west-3:543553163241:pipeline/trainingpipelinerainau',\n",
       " 'ResponseMetadata': {'RequestId': '437cc514-8e40-415a-9801-a2e042900c8e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '437cc514-8e40-415a-9801-a2e042900c8e',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '90',\n",
       "   'date': 'Mon, 24 Jan 2022 16:34:20 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "adaa21aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ba211d14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "471825bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regression_metrics': {'accuracy': 0.8363423076923077, 'precision': 0.6699066830139103, 'recall': 0.44946899841182447, 'f1_score': 0.5366143977939883}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "uri = evaluation_s3_uri\n",
    "\n",
    "obj = s3_client.get_object(Bucket=uri.split(\"/\")[2], Key=\"/\".join(uri.split(\"/\")[3:]))\n",
    "\n",
    "report = json.loads(obj['Body'].read())\n",
    "#df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "\n",
    "\n",
    "#with open(evaluation_output_uris[\"evaluation\"]+ \"/evaluation.json\") as jf:\n",
    "#    report = json.load(jf)\n",
    "    \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95949cc7",
   "metadata": {},
   "source": [
    "### The model has a high enough accuracy and we like the other stats (not really but let's say so), so we can go and manually approve the model to join the registry.\n",
    "\n",
    "We can now develop a deployment script that will get our model and deploy it.\n",
    "\n",
    "P.s. If we don't like the manual approval of models we can change `model_approval_status` to `'Approved'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0a8340dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 13, done.\u001b[K\n",
      "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 7 (delta 6), reused 7 (delta 6), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (7/7), 579 bytes | 193.00 KiB/s, done.\n",
      "From https://github.com/gmguarino/sagemaker-pipeline-rain-australia-build\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   241a27f..a41dda7  main       -> origin/main\n",
      "Updating 241a27f..a41dda7\n",
      "Fast-forward\n",
      " deploy_utils.py              | 8 \u001b[32m++\u001b[m\u001b[31m------\u001b[m\n",
      " pipelines/rain/inference.py  | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " pipelines/rain/preprocess.py | 1 \u001b[32m+\u001b[m\n",
      " 3 files changed, 4 insertions(+), 7 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "914e8815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebuild-buildspec.yml  pipelines\t\t\t  README.md  tests\r\n",
      "deploy_utils.py\t\t rain-au-notebook-dev.ipynb\t  setup.cfg\r\n",
      "LICENSE\t\t\t rain-au-notebook-pipeline.ipynb  setup.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e7f55e",
   "metadata": {},
   "source": [
    "We can get some helper code from `deploy_utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "978b4e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deploy_utils import get_latest_approved_package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b73f3",
   "metadata": {},
   "source": [
    "Let's print out the description for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5fede744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageGroupName': 'RainAuModel',\n",
       " 'ModelPackageVersion': 1,\n",
       " 'ModelPackageArn': 'arn:aws:sagemaker:eu-west-3:543553163241:model-package/rainaumodel/1',\n",
       " 'CreationTime': datetime.datetime(2022, 1, 24, 16, 50, 29, 451000, tzinfo=tzlocal()),\n",
       " 'InferenceSpecification': {'Containers': [{'Image': '659782779980.dkr.ecr.eu-west-3.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "    'ImageDigest': 'sha256:fc21b1b187c1980fa792c9bd648c34d60e8622ad0963c140a8eb60ceef2fc549',\n",
       "    'ModelDataUrl': 's3://sagemaker-eu-west-3-543553163241/pipelines-83o705b9nxst-sklearnRepackModel-gO4lDripba/output/model.tar.gz',\n",
       "    'Environment': {'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "     'SAGEMAKER_PROGRAM': 'preprocess.py',\n",
       "     'SAGEMAKER_REGION': 'eu-west-3',\n",
       "     'SAGEMAKER_SUBMIT_DIRECTORY': 's3://sagemaker-eu-west-3-543553163241/sagemaker-scikit-learn-2022-01-24-15-41-47-116/sourcedir.tar.gz'}},\n",
       "   {'Image': '763104351884.dkr.ecr.eu-west-3.amazonaws.com/pytorch-inference:1.8.0-cpu-py3',\n",
       "    'ImageDigest': 'sha256:7c4c7ea4f0e8cfe25441880fe23ffcac5997c03e00358e4e22567eb160f8537d',\n",
       "    'ModelDataUrl': 's3://sagemaker-eu-west-3-543553163241/pipelines-83o705b9nxst-rain-au-inference-mo-G2l09wA3M2/output/model.tar.gz',\n",
       "    'Environment': {'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "     'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "     'SAGEMAKER_REGION': 'eu-west-3',\n",
       "     'SAGEMAKER_SUBMIT_DIRECTORY': 'file://pipelines/rain/'}}],\n",
       "  'SupportedTransformInstanceTypes': ['ml.m5.xlarge'],\n",
       "  'SupportedRealtimeInferenceInstanceTypes': ['ml.m5.large', 'ml.m5.xlarge'],\n",
       "  'SupportedContentTypes': ['application/json'],\n",
       "  'SupportedResponseMIMETypes': ['application/json']},\n",
       " 'ModelPackageStatus': 'Completed',\n",
       " 'ModelPackageStatusDetails': {'ValidationStatuses': [],\n",
       "  'ImageScanStatuses': []},\n",
       " 'CertifyForMarketplace': False,\n",
       " 'ModelApprovalStatus': 'Approved',\n",
       " 'MetadataProperties': {'GeneratedBy': 'arn:aws:sagemaker:eu-west-3:543553163241:pipeline/trainingpipelinerainau/execution/83o705b9nxst'},\n",
       " 'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
       "    'S3Uri': 's3://sagemaker-eu-west-3-543553163241/EvaluatePerformance-c80c4963fbba66f1a47de8c9c9893092/output/evaluation/evaluation.json'}},\n",
       "  'Bias': {},\n",
       "  'Explainability': {}},\n",
       " 'LastModifiedTime': datetime.datetime(2022, 1, 24, 17, 27, 47, 232000, tzinfo=tzlocal()),\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-3:543553163241:user-profile/d-1tq2bfnwfcru/studio-user-giuseppe',\n",
       "  'UserProfileName': 'studio-user-giuseppe',\n",
       "  'DomainId': 'd-1tq2bfnwfcru'},\n",
       " 'ApprovalDescription': 'i like this model',\n",
       " 'ResponseMetadata': {'RequestId': '1829c225-974b-4b76-877a-cdd4a7d637b6',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1829c225-974b-4b76-877a-cdd4a7d637b6',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2319',\n",
       "   'date': 'Mon, 24 Jan 2022 17:57:51 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# model_package_group_name was defined at the beginning of the pipeline definition \n",
    "package = get_latest_approved_package(model_package_group_name) \n",
    "\n",
    "model_pak_arn = package[\"ModelPackageArn\"]\n",
    "\n",
    "model_description = sm_client.describe_model_package(ModelPackageName=model_pak_arn)\n",
    "\n",
    "model_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "40564bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now load it using the sagemaker ModelPackage class\n",
    "from sagemaker import ModelPackage\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b0ad46da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "deploy_model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_pak_arn,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "endpoint_name = \"RainAu-test-deployment-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "# Deploy the model\n",
    "deploy_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bca67f",
   "metadata": {},
   "source": [
    "# Create a Predictor from an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f255e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "predictor = Predictor(endpoint_name=endpoint_name)\n",
    "# Can use this to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "08202d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-west-3-543553163241/RainAuProcessing-e1284f24b646de14ac1aa45ecdee541d/output/test'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load test data\n",
    "test_data_bucket = processing_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][2][\"S3Output\"][\"S3Uri\"]\n",
    "test_data_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "797b3271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RainTomorrow</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>year</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.196331</td>\n",
       "      <td>-0.265806</td>\n",
       "      <td>-0.764067</td>\n",
       "      <td>-0.203581</td>\n",
       "      <td>-0.622806</td>\n",
       "      <td>-0.503988</td>\n",
       "      <td>1.258262</td>\n",
       "      <td>-0.530619</td>\n",
       "      <td>1.328766</td>\n",
       "      <td>...</td>\n",
       "      <td>1.025756</td>\n",
       "      <td>-0.336969</td>\n",
       "      <td>-0.463168</td>\n",
       "      <td>-0.623142</td>\n",
       "      <td>-0.529795</td>\n",
       "      <td>0.878855</td>\n",
       "      <td>-1.244369</td>\n",
       "      <td>-0.682476</td>\n",
       "      <td>-1.317684</td>\n",
       "      <td>0.523473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.365915</td>\n",
       "      <td>1.100882</td>\n",
       "      <td>0.505625</td>\n",
       "      <td>0.821481</td>\n",
       "      <td>-0.119472</td>\n",
       "      <td>-1.156687</td>\n",
       "      <td>-0.446007</td>\n",
       "      <td>1.298526</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149133</td>\n",
       "      <td>1.561680</td>\n",
       "      <td>0.901326</td>\n",
       "      <td>0.179704</td>\n",
       "      <td>1.887521</td>\n",
       "      <td>0.878855</td>\n",
       "      <td>1.211519</td>\n",
       "      <td>-0.682476</td>\n",
       "      <td>1.255292</td>\n",
       "      <td>-0.601030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.857881</td>\n",
       "      <td>-1.711038</td>\n",
       "      <td>-1.427128</td>\n",
       "      <td>-0.155903</td>\n",
       "      <td>-0.119472</td>\n",
       "      <td>0.148710</td>\n",
       "      <td>0.406127</td>\n",
       "      <td>-2.054906</td>\n",
       "      <td>0.221549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149133</td>\n",
       "      <td>0.137693</td>\n",
       "      <td>-1.284966</td>\n",
       "      <td>-1.338404</td>\n",
       "      <td>-0.529795</td>\n",
       "      <td>0.090732</td>\n",
       "      <td>-0.725379</td>\n",
       "      <td>-1.198979</td>\n",
       "      <td>-1.261688</td>\n",
       "      <td>-0.601030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.266612</td>\n",
       "      <td>-1.051258</td>\n",
       "      <td>0.011856</td>\n",
       "      <td>-0.275097</td>\n",
       "      <td>0.069278</td>\n",
       "      <td>-0.286422</td>\n",
       "      <td>-0.872075</td>\n",
       "      <td>1.603383</td>\n",
       "      <td>-0.664226</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.042425</td>\n",
       "      <td>1.561680</td>\n",
       "      <td>-0.122045</td>\n",
       "      <td>-0.083046</td>\n",
       "      <td>-0.529795</td>\n",
       "      <td>1.272917</td>\n",
       "      <td>-1.434333</td>\n",
       "      <td>0.023080</td>\n",
       "      <td>-0.803968</td>\n",
       "      <td>1.199371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.787600</td>\n",
       "      <td>0.205466</td>\n",
       "      <td>0.181148</td>\n",
       "      <td>-0.108226</td>\n",
       "      <td>-0.119472</td>\n",
       "      <td>0.148710</td>\n",
       "      <td>1.045228</td>\n",
       "      <td>-0.378190</td>\n",
       "      <td>1.328766</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.604113</td>\n",
       "      <td>1.087018</td>\n",
       "      <td>-0.060022</td>\n",
       "      <td>0.106718</td>\n",
       "      <td>1.887521</td>\n",
       "      <td>1.666978</td>\n",
       "      <td>0.692529</td>\n",
       "      <td>1.245139</td>\n",
       "      <td>-1.261688</td>\n",
       "      <td>-0.601030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RainTomorrow  Location   MinTemp   MaxTemp  Rainfall  Evaporation  \\\n",
       "0             0 -0.196331 -0.265806 -0.764067 -0.203581    -0.622806   \n",
       "1             0  0.365915  1.100882  0.505625  0.821481    -0.119472   \n",
       "2             0  0.857881 -1.711038 -1.427128 -0.155903    -0.119472   \n",
       "3             1 -0.266612 -1.051258  0.011856 -0.275097     0.069278   \n",
       "4             0  0.787600  0.205466  0.181148 -0.108226    -0.119472   \n",
       "\n",
       "   Sunshine  WindGustDir  WindGustSpeed  WindDir9am  ...  Cloud9am  Cloud3pm  \\\n",
       "0 -0.503988     1.258262      -0.530619    1.328766  ...  1.025756 -0.336969   \n",
       "1 -1.156687    -0.446007       1.298526    0.000105  ...  0.149133  1.561680   \n",
       "2  0.148710     0.406127      -2.054906    0.221549  ...  0.149133  0.137693   \n",
       "3 -0.286422    -0.872075       1.603383   -0.664226  ... -2.042425  1.561680   \n",
       "4  0.148710     1.045228      -0.378190    1.328766  ... -1.604113  1.087018   \n",
       "\n",
       "    Temp9am   Temp3pm  RainToday      year  month_sin  month_cos   day_sin  \\\n",
       "0 -0.463168 -0.623142  -0.529795  0.878855  -1.244369  -0.682476 -1.317684   \n",
       "1  0.901326  0.179704   1.887521  0.878855   1.211519  -0.682476  1.255292   \n",
       "2 -1.284966 -1.338404  -0.529795  0.090732  -0.725379  -1.198979 -1.261688   \n",
       "3 -0.122045 -0.083046  -0.529795  1.272917  -1.434333   0.023080 -0.803968   \n",
       "4 -0.060022  0.106718   1.887521  1.666978   0.692529   1.245139 -1.261688   \n",
       "\n",
       "    day_cos  \n",
       "0  0.523473  \n",
       "1 -0.601030  \n",
       "2 -0.601030  \n",
       "3  1.199371  \n",
       "4 -0.601030  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(test_data_bucket + \"/test.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "629facbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "y = test_data[\"RainTomorrow\"].values\n",
    "test_payload = test_data.drop(columns=\"RainTomorrow\").iloc[:batch_size].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "66478e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_payload = {\"data\":test_payload} \n",
    "\n",
    "#There is an import error in inference.py FIX!\n",
    "#p = predictor.predict(json.dumps(json_payload), initial_args={\"ContentType\": \"application/json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ec37c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add rain-au-notebook-dev.ipynb rain-au-notebook-pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "edbd42e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\r\n",
      "Your branch is ahead of 'origin/main' by 1 commit.\r\n",
      "  (use \"git push\" to publish your local commits)\r\n",
      "\r\n",
      "Changes not staged for commit:\r\n",
      "  (use \"git add <file>...\" to update what will be committed)\r\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\r\n",
      "\t\u001b[31mmodified:   rain-au-notebook-pipeline.ipynb\u001b[m\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\t\u001b[31m.ipynb_checkpoints/\u001b[m\r\n",
      "\t\u001b[31mpipelines/rain/_repack_model.py\u001b[m\r\n",
      "\r\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"added notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab364ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p37",
   "language": "python",
   "name": "conda_pytorch_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
